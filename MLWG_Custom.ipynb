{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuFIt3CBZWbt",
        "outputId": "ce95f9d5-17df-48bc-cf47-48b12deded87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4bIrXOmc198",
        "outputId": "cfaf6a11-d485-4dae-a07d-7feaec8fe913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import MovieLens100K"
      ],
      "metadata": {
        "id": "-EPjt7Gcc7j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MovieLens100K(root='/tmp/MovieLens')\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xlEjbggdAR1",
        "outputId": "698d06e7-f5b2-44ce-80b2-f032ae1ef03b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
            "Extracting /tmp/MovieLens/ml-100k.zip\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MovieLens100K()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset[0]\n",
        "\n",
        "# Explore the data\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcAgXMrYdDuh",
        "outputId": "ac278e94-d5e0-43ac-adaf-1597c8fa3132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HeteroData(\n",
            "  movie={ x=[1682, 18] },\n",
            "  user={ x=[943, 24] },\n",
            "  (user, rates, movie)={\n",
            "    edge_index=[2, 80000],\n",
            "    rating=[80000],\n",
            "    time=[80000],\n",
            "    edge_label_index=[2, 20000],\n",
            "    edge_label=[20000],\n",
            "  },\n",
            "  (movie, rated_by, user)={\n",
            "    edge_index=[2, 80000],\n",
            "    rating=[80000],\n",
            "    time=[80000],\n",
            "  }\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "\n",
        "if 'edge_label_index' in data['user', 'rates', 'movie'] and 'edge_label' in data['user', 'rates', 'movie']:\n",
        "    del data['user', 'rates', 'movie'].edge_label_index\n",
        "    del data['user', 'rates', 'movie'].edge_label\n",
        "\n",
        "# Verify the dataset after cleaning\n",
        "print(\"Dataset after removing edge_label and edge_label_index:\")\n",
        "print(data)\n",
        "\n",
        "\n",
        "splitter = RandomLinkSplit(\n",
        "    num_val=0.15,  # 15% validation edges (reduced for more training data)\n",
        "    num_test=0.15,  # 15% test edges\n",
        "    is_undirected=True,  # Ensure this matches the graph type\n",
        "    add_negative_train_samples=True,  # Enable negative sampling\n",
        "    neg_sampling_ratio=1.0,  # Ratio of negative samples to positive samples\n",
        "    edge_types=('user', 'rates', 'movie'),  # Specify the edge type to split\n",
        "    rev_edge_types=('movie', 'rated_by', 'user'),  # Reverse edge type\n",
        "    disjoint_train_ratio=0.1  # Ensure disjoint splits between train/val/test\n",
        ")\n",
        "\n",
        "# Perform the split\n",
        "train_data, val_data, test_data = splitter(data)\n",
        "\n",
        "# Print details about the split\n",
        "print(\"\\n--- Split Summary ---\")\n",
        "print(f\"Training Data Edge Index Shape: {train_data['user', 'rates', 'movie'].edge_index.shape}\")\n",
        "print(f\"Validation Data Edge Index Shape: {val_data['user', 'rates', 'movie'].edge_index.shape}\")\n",
        "print(f\"Test Data Edge Index Shape: {test_data['user', 'rates', 'movie'].edge_index.shape}\")\n",
        "\n",
        "# Add additional checks if needed\n",
        "print(f\"\\nTraining Data Edge Index: {train_data['user', 'rates', 'movie'].edge_index}\")\n",
        "print(f\"Validation Data Edge Index: {val_data['user', 'rates', 'movie'].edge_index}\")\n",
        "print(f\"Test Data Edge Index: {test_data['user', 'rates', 'movie'].edge_index}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRIj_lV2dG-O",
        "outputId": "59281000-41b7-4295-a9ff-5b2b2145ec8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset after removing edge_label and edge_label_index:\n",
            "HeteroData(\n",
            "  movie={ x=[1682, 18] },\n",
            "  user={ x=[943, 24] },\n",
            "  (user, rates, movie)={\n",
            "    edge_index=[2, 80000],\n",
            "    rating=[80000],\n",
            "    time=[80000],\n",
            "  },\n",
            "  (movie, rated_by, user)={\n",
            "    edge_index=[2, 80000],\n",
            "    rating=[80000],\n",
            "    time=[80000],\n",
            "  }\n",
            ")\n",
            "\n",
            "--- Split Summary ---\n",
            "Training Data Edge Index Shape: torch.Size([2, 50400])\n",
            "Validation Data Edge Index Shape: torch.Size([2, 56000])\n",
            "Test Data Edge Index Shape: torch.Size([2, 68000])\n",
            "\n",
            "Training Data Edge Index: tensor([[845, 710, 602,  ...,   6, 770, 297],\n",
            "        [ 38, 113, 448,  ..., 355, 651, 356]])\n",
            "Validation Data Edge Index: tensor([[795, 639, 498,  ...,   6, 770, 297],\n",
            "        [177, 549, 207,  ..., 355, 651, 356]])\n",
            "Test Data Edge Index: tensor([[795, 639, 498,  ..., 915, 659, 663],\n",
            "        [177, 549, 207,  ..., 133, 384, 195]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "from torch_geometric.nn import SAGEConv, to_hetero\n",
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        # Input to hidden layer\n",
        "        self.convs.append(SAGEConv((-1, -1), hidden_channels))\n",
        "\n",
        "        # Hidden to hidden layers\n",
        "        for _ in range(1, num_layers - 1):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "\n",
        "        # Hidden to output layer\n",
        "        self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "            x = self.dropout(x)  # Apply dropout after each hidden layer\n",
        "\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, data: HeteroData, hidden_channels):\n",
        "        super().__init__()\n",
        "        # Extract the number of nodes for users and movies\n",
        "        num_users = data['user'].num_nodes\n",
        "        num_movies = data['movie'].num_nodes\n",
        "        user_feat_dim = data['user'].x.size(1)\n",
        "        movie_feat_dim = data['movie'].x.size(1)\n",
        "\n",
        "        # Learnable embeddings for users and movies\n",
        "        self.user_emb = torch.nn.Embedding(num_users, hidden_channels)\n",
        "        self.movie_emb = torch.nn.Embedding(num_movies, hidden_channels)\n",
        "\n",
        "        # Project user and movie features to the hidden dimension\n",
        "        self.user_lin = torch.nn.Linear(user_feat_dim, hidden_channels)\n",
        "        self.movie_lin = torch.nn.Linear(movie_feat_dim, hidden_channels)\n",
        "\n",
        "        self.gnn = GNN(hidden_channels)\n",
        "\n",
        "        self.gnn = to_hetero(self.gnn, metadata=data.metadata())\n",
        "\n",
        "    def forward(self, data: HeteroData) -> dict:\n",
        "        # Generate initial embeddings for users and movies\n",
        "        x_dict = {\n",
        "            \"user\": self.user_lin(data[\"user\"].x) + self.user_emb.weight,  # Combine user features and embeddings\n",
        "            \"movie\": self.movie_lin(data[\"movie\"].x) + self.movie_emb.weight,  # Combine movie features and embeddings\n",
        "        }\n",
        "\n",
        "        # Apply the heterogeneous GNN\n",
        "        x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
        "        return x_dict\n",
        "\n",
        "\n",
        "def bpr_loss(user_emb, pos_movie_emb, neg_movie_emb):\n",
        "    pos_scores = (user_emb * pos_movie_emb).sum(dim=-1)\n",
        "    neg_scores = (user_emb * neg_movie_emb).sum(dim=-1)\n",
        "    loss = -torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def train_bpr(data: HeteroData, model, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    x_dict = model(data)\n",
        "    user_emb = x_dict['user']\n",
        "    movie_emb = x_dict['movie']\n",
        "\n",
        "    # Positive and negative edge sampling\n",
        "    pos_edge_index = data['user', 'rates', 'movie'].edge_index\n",
        "    neg_edge_index = torch.randint(\n",
        "        low=0, high=data['movie'].num_nodes, size=pos_edge_index.size(), device=pos_edge_index.device\n",
        "    )\n",
        "\n",
        "    # Extract embeddings for positive and negative edges\n",
        "    user_emb = user_emb[pos_edge_index[0]]\n",
        "    pos_movie_emb = movie_emb[pos_edge_index[1]]\n",
        "    neg_movie_emb = movie_emb[neg_edge_index[1]]\n",
        "\n",
        "    # Compute BPR loss\n",
        "    loss = bpr_loss(user_emb, pos_movie_emb, neg_movie_emb)\n",
        "    loss.backward()\n",
        "    #print(\"User Embedding Gradients:\", model.user_emb.weight.grad)\n",
        "    #print(\"Movie Embedding Gradients:\", model.movie_emb.weight.grad)\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(data: HeteroData, model, k=20):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_dict = model(data)\n",
        "        user_emb = x_dict['user']\n",
        "        movie_emb = x_dict['movie']\n",
        "\n",
        "        # Extract edge indices for positive edges\n",
        "        pos_edge_index = data['user', 'rates', 'movie'].edge_index\n",
        "\n",
        "        # Compute scores for all movies for each user\n",
        "        scores = torch.matmul(user_emb, movie_emb.t())  # [num_users, num_movies]\n",
        "\n",
        "        # Extract ground-truth positive edges\n",
        "        user_ids = pos_edge_index[0]  # Users in positive edges\n",
        "        movie_ids = pos_edge_index[1]  # Movies in positive edges\n",
        "\n",
        "        # Initialize metrics\n",
        "        precision = 0.0\n",
        "        recall = 0.0\n",
        "        ndcg = 0.0\n",
        "\n",
        "        for user_id in user_ids.unique():\n",
        "            user_scores = scores[user_id]  # Scores for the current user [num_movies]\n",
        "            true_movie_ids = movie_ids[user_ids == user_id]  # Ground truth movies for the user\n",
        "\n",
        "            # Skip users with no positive edges\n",
        "            if true_movie_ids.numel() == 0:\n",
        "                continue\n",
        "\n",
        "            # Get top-K movie indices\n",
        "            _, top_k_indices = torch.topk(user_scores, k)\n",
        "\n",
        "            # Compute Precision@K\n",
        "            hits = torch.isin(top_k_indices, true_movie_ids).sum().item()\n",
        "            precision += hits / k\n",
        "\n",
        "            # Compute Recall@K\n",
        "            recall += hits / true_movie_ids.numel()\n",
        "\n",
        "            # Compute nDCG@K\n",
        "            gains = torch.isin(top_k_indices, true_movie_ids).float()\n",
        "            discounts = torch.log2(torch.arange(2, k + 2).float())\n",
        "            dcg = (gains / discounts).sum()\n",
        "            idcg = (1.0 / discounts[: min(k, true_movie_ids.numel())]).sum()\n",
        "            ndcg += dcg / idcg\n",
        "\n",
        "        # Average metrics over all users\n",
        "        num_users = user_ids.unique().numel()\n",
        "        precision /= num_users\n",
        "        recall /= num_users\n",
        "        ndcg /= num_users\n",
        "\n",
        "    return precision, recall, ndcg\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "hidden_channels = 64\n",
        "model = Model(data, hidden_channels=hidden_channels)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_bpr(train_data, model, optimizer)\n",
        "    precision, recall, ndcg = evaluate(val_data, model, k=20)\n",
        "\n",
        "    print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Precision@20: {precision:.4f}, Recall@20: {recall:.4f}, nDCG@20: {ndcg:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tmsonqHdVyR",
        "outputId": "48340d41-a6b2-41cc-e85c-bab0fa24d8d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Train Loss: 0.7524, Precision@20: 0.0249, Recall@20: 0.0096, nDCG@20: 0.0211\n",
            "Epoch 1, Train Loss: 0.7122, Precision@20: 0.0284, Recall@20: 0.0105, nDCG@20: 0.0240\n",
            "Epoch 2, Train Loss: 0.6782, Precision@20: 0.0324, Recall@20: 0.0126, nDCG@20: 0.0282\n",
            "Epoch 3, Train Loss: 0.6539, Precision@20: 0.0356, Recall@20: 0.0139, nDCG@20: 0.0316\n",
            "Epoch 4, Train Loss: 0.6290, Precision@20: 0.0415, Recall@20: 0.0163, nDCG@20: 0.0364\n",
            "Epoch 5, Train Loss: 0.6091, Precision@20: 0.0466, Recall@20: 0.0182, nDCG@20: 0.0400\n",
            "Epoch 6, Train Loss: 0.6006, Precision@20: 0.0499, Recall@20: 0.0200, nDCG@20: 0.0421\n",
            "Epoch 7, Train Loss: 0.5763, Precision@20: 0.0534, Recall@20: 0.0219, nDCG@20: 0.0445\n",
            "Epoch 8, Train Loss: 0.5698, Precision@20: 0.0578, Recall@20: 0.0238, nDCG@20: 0.0477\n",
            "Epoch 9, Train Loss: 0.5539, Precision@20: 0.0641, Recall@20: 0.0264, nDCG@20: 0.0527\n",
            "Epoch 10, Train Loss: 0.5518, Precision@20: 0.0688, Recall@20: 0.0284, nDCG@20: 0.0570\n",
            "Epoch 11, Train Loss: 0.5294, Precision@20: 0.0748, Recall@20: 0.0311, nDCG@20: 0.0624\n",
            "Epoch 12, Train Loss: 0.5243, Precision@20: 0.0787, Recall@20: 0.0330, nDCG@20: 0.0664\n",
            "Epoch 13, Train Loss: 0.5187, Precision@20: 0.0837, Recall@20: 0.0346, nDCG@20: 0.0712\n",
            "Epoch 14, Train Loss: 0.5091, Precision@20: 0.0884, Recall@20: 0.0371, nDCG@20: 0.0765\n",
            "Epoch 15, Train Loss: 0.5009, Precision@20: 0.0937, Recall@20: 0.0391, nDCG@20: 0.0827\n",
            "Epoch 16, Train Loss: 0.4924, Precision@20: 0.1015, Recall@20: 0.0422, nDCG@20: 0.0914\n",
            "Epoch 17, Train Loss: 0.4856, Precision@20: 0.1106, Recall@20: 0.0468, nDCG@20: 0.1019\n",
            "Epoch 18, Train Loss: 0.4807, Precision@20: 0.1212, Recall@20: 0.0515, nDCG@20: 0.1137\n",
            "Epoch 19, Train Loss: 0.4728, Precision@20: 0.1323, Recall@20: 0.0565, nDCG@20: 0.1277\n",
            "Epoch 20, Train Loss: 0.4682, Precision@20: 0.1463, Recall@20: 0.0623, nDCG@20: 0.1422\n",
            "Epoch 21, Train Loss: 0.4606, Precision@20: 0.1566, Recall@20: 0.0671, nDCG@20: 0.1533\n",
            "Epoch 22, Train Loss: 0.4560, Precision@20: 0.1647, Recall@20: 0.0716, nDCG@20: 0.1622\n",
            "Epoch 23, Train Loss: 0.4493, Precision@20: 0.1709, Recall@20: 0.0752, nDCG@20: 0.1715\n",
            "Epoch 24, Train Loss: 0.4482, Precision@20: 0.1764, Recall@20: 0.0789, nDCG@20: 0.1797\n",
            "Epoch 25, Train Loss: 0.4488, Precision@20: 0.1830, Recall@20: 0.0833, nDCG@20: 0.1890\n",
            "Epoch 26, Train Loss: 0.4401, Precision@20: 0.1893, Recall@20: 0.0878, nDCG@20: 0.1983\n",
            "Epoch 27, Train Loss: 0.4384, Precision@20: 0.1961, Recall@20: 0.0925, nDCG@20: 0.2078\n",
            "Epoch 28, Train Loss: 0.4323, Precision@20: 0.2010, Recall@20: 0.0960, nDCG@20: 0.2155\n",
            "Epoch 29, Train Loss: 0.4255, Precision@20: 0.2072, Recall@20: 0.0998, nDCG@20: 0.2228\n",
            "Epoch 30, Train Loss: 0.4225, Precision@20: 0.2125, Recall@20: 0.1026, nDCG@20: 0.2283\n",
            "Epoch 31, Train Loss: 0.4221, Precision@20: 0.2166, Recall@20: 0.1047, nDCG@20: 0.2331\n",
            "Epoch 32, Train Loss: 0.4151, Precision@20: 0.2210, Recall@20: 0.1068, nDCG@20: 0.2384\n",
            "Epoch 33, Train Loss: 0.4147, Precision@20: 0.2256, Recall@20: 0.1097, nDCG@20: 0.2431\n",
            "Epoch 34, Train Loss: 0.4116, Precision@20: 0.2293, Recall@20: 0.1115, nDCG@20: 0.2469\n",
            "Epoch 35, Train Loss: 0.4098, Precision@20: 0.2313, Recall@20: 0.1123, nDCG@20: 0.2502\n",
            "Epoch 36, Train Loss: 0.4065, Precision@20: 0.2326, Recall@20: 0.1127, nDCG@20: 0.2530\n",
            "Epoch 37, Train Loss: 0.4039, Precision@20: 0.2350, Recall@20: 0.1139, nDCG@20: 0.2567\n",
            "Epoch 38, Train Loss: 0.4038, Precision@20: 0.2365, Recall@20: 0.1140, nDCG@20: 0.2594\n",
            "Epoch 39, Train Loss: 0.3982, Precision@20: 0.2385, Recall@20: 0.1152, nDCG@20: 0.2627\n",
            "Epoch 40, Train Loss: 0.4002, Precision@20: 0.2399, Recall@20: 0.1158, nDCG@20: 0.2647\n",
            "Epoch 41, Train Loss: 0.3940, Precision@20: 0.2427, Recall@20: 0.1171, nDCG@20: 0.2677\n",
            "Epoch 42, Train Loss: 0.3922, Precision@20: 0.2461, Recall@20: 0.1186, nDCG@20: 0.2708\n",
            "Epoch 43, Train Loss: 0.3896, Precision@20: 0.2465, Recall@20: 0.1190, nDCG@20: 0.2724\n",
            "Epoch 44, Train Loss: 0.3893, Precision@20: 0.2481, Recall@20: 0.1200, nDCG@20: 0.2751\n",
            "Epoch 45, Train Loss: 0.3935, Precision@20: 0.2510, Recall@20: 0.1216, nDCG@20: 0.2777\n",
            "Epoch 46, Train Loss: 0.3882, Precision@20: 0.2527, Recall@20: 0.1228, nDCG@20: 0.2797\n",
            "Epoch 47, Train Loss: 0.3865, Precision@20: 0.2533, Recall@20: 0.1228, nDCG@20: 0.2813\n",
            "Epoch 48, Train Loss: 0.3845, Precision@20: 0.2543, Recall@20: 0.1230, nDCG@20: 0.2827\n",
            "Epoch 49, Train Loss: 0.3822, Precision@20: 0.2545, Recall@20: 0.1227, nDCG@20: 0.2846\n",
            "Epoch 50, Train Loss: 0.3807, Precision@20: 0.2565, Recall@20: 0.1236, nDCG@20: 0.2870\n",
            "Epoch 51, Train Loss: 0.3823, Precision@20: 0.2589, Recall@20: 0.1254, nDCG@20: 0.2893\n",
            "Epoch 52, Train Loss: 0.3785, Precision@20: 0.2602, Recall@20: 0.1264, nDCG@20: 0.2908\n",
            "Epoch 53, Train Loss: 0.3750, Precision@20: 0.2609, Recall@20: 0.1268, nDCG@20: 0.2923\n",
            "Epoch 54, Train Loss: 0.3725, Precision@20: 0.2617, Recall@20: 0.1279, nDCG@20: 0.2934\n",
            "Epoch 55, Train Loss: 0.3738, Precision@20: 0.2627, Recall@20: 0.1289, nDCG@20: 0.2947\n",
            "Epoch 56, Train Loss: 0.3749, Precision@20: 0.2641, Recall@20: 0.1297, nDCG@20: 0.2963\n",
            "Epoch 57, Train Loss: 0.3732, Precision@20: 0.2648, Recall@20: 0.1307, nDCG@20: 0.2977\n",
            "Epoch 58, Train Loss: 0.3705, Precision@20: 0.2653, Recall@20: 0.1316, nDCG@20: 0.2990\n",
            "Epoch 59, Train Loss: 0.3679, Precision@20: 0.2655, Recall@20: 0.1318, nDCG@20: 0.2992\n",
            "Epoch 60, Train Loss: 0.3665, Precision@20: 0.2667, Recall@20: 0.1328, nDCG@20: 0.3007\n",
            "Epoch 61, Train Loss: 0.3653, Precision@20: 0.2676, Recall@20: 0.1340, nDCG@20: 0.3020\n",
            "Epoch 62, Train Loss: 0.3652, Precision@20: 0.2683, Recall@20: 0.1346, nDCG@20: 0.3030\n",
            "Epoch 63, Train Loss: 0.3691, Precision@20: 0.2678, Recall@20: 0.1346, nDCG@20: 0.3028\n",
            "Epoch 64, Train Loss: 0.3627, Precision@20: 0.2678, Recall@20: 0.1351, nDCG@20: 0.3037\n",
            "Epoch 65, Train Loss: 0.3620, Precision@20: 0.2688, Recall@20: 0.1354, nDCG@20: 0.3043\n",
            "Epoch 66, Train Loss: 0.3596, Precision@20: 0.2697, Recall@20: 0.1362, nDCG@20: 0.3053\n",
            "Epoch 67, Train Loss: 0.3580, Precision@20: 0.2716, Recall@20: 0.1370, nDCG@20: 0.3071\n",
            "Epoch 68, Train Loss: 0.3557, Precision@20: 0.2725, Recall@20: 0.1374, nDCG@20: 0.3083\n",
            "Epoch 69, Train Loss: 0.3523, Precision@20: 0.2730, Recall@20: 0.1370, nDCG@20: 0.3090\n",
            "Epoch 70, Train Loss: 0.3554, Precision@20: 0.2747, Recall@20: 0.1382, nDCG@20: 0.3105\n",
            "Epoch 71, Train Loss: 0.3517, Precision@20: 0.2750, Recall@20: 0.1386, nDCG@20: 0.3113\n",
            "Epoch 72, Train Loss: 0.3504, Precision@20: 0.2784, Recall@20: 0.1408, nDCG@20: 0.3147\n",
            "Epoch 73, Train Loss: 0.3493, Precision@20: 0.2800, Recall@20: 0.1423, nDCG@20: 0.3171\n",
            "Epoch 74, Train Loss: 0.3487, Precision@20: 0.2814, Recall@20: 0.1435, nDCG@20: 0.3189\n",
            "Epoch 75, Train Loss: 0.3462, Precision@20: 0.2826, Recall@20: 0.1447, nDCG@20: 0.3205\n",
            "Epoch 76, Train Loss: 0.3499, Precision@20: 0.2840, Recall@20: 0.1458, nDCG@20: 0.3225\n",
            "Epoch 77, Train Loss: 0.3443, Precision@20: 0.2848, Recall@20: 0.1468, nDCG@20: 0.3243\n",
            "Epoch 78, Train Loss: 0.3428, Precision@20: 0.2861, Recall@20: 0.1475, nDCG@20: 0.3258\n",
            "Epoch 79, Train Loss: 0.3425, Precision@20: 0.2877, Recall@20: 0.1487, nDCG@20: 0.3283\n",
            "Epoch 80, Train Loss: 0.3376, Precision@20: 0.2891, Recall@20: 0.1503, nDCG@20: 0.3300\n",
            "Epoch 81, Train Loss: 0.3394, Precision@20: 0.2898, Recall@20: 0.1513, nDCG@20: 0.3309\n",
            "Epoch 82, Train Loss: 0.3375, Precision@20: 0.2911, Recall@20: 0.1522, nDCG@20: 0.3325\n",
            "Epoch 83, Train Loss: 0.3370, Precision@20: 0.2935, Recall@20: 0.1538, nDCG@20: 0.3344\n",
            "Epoch 84, Train Loss: 0.3316, Precision@20: 0.2949, Recall@20: 0.1553, nDCG@20: 0.3363\n",
            "Epoch 85, Train Loss: 0.3309, Precision@20: 0.2967, Recall@20: 0.1563, nDCG@20: 0.3378\n",
            "Epoch 86, Train Loss: 0.3299, Precision@20: 0.2993, Recall@20: 0.1575, nDCG@20: 0.3398\n",
            "Epoch 87, Train Loss: 0.3292, Precision@20: 0.2998, Recall@20: 0.1581, nDCG@20: 0.3408\n",
            "Epoch 88, Train Loss: 0.3259, Precision@20: 0.3021, Recall@20: 0.1593, nDCG@20: 0.3432\n",
            "Epoch 89, Train Loss: 0.3260, Precision@20: 0.3030, Recall@20: 0.1596, nDCG@20: 0.3437\n",
            "Epoch 90, Train Loss: 0.3269, Precision@20: 0.3049, Recall@20: 0.1601, nDCG@20: 0.3449\n",
            "Epoch 91, Train Loss: 0.3251, Precision@20: 0.3070, Recall@20: 0.1611, nDCG@20: 0.3460\n",
            "Epoch 92, Train Loss: 0.3208, Precision@20: 0.3095, Recall@20: 0.1624, nDCG@20: 0.3481\n",
            "Epoch 93, Train Loss: 0.3218, Precision@20: 0.3102, Recall@20: 0.1636, nDCG@20: 0.3489\n",
            "Epoch 94, Train Loss: 0.3184, Precision@20: 0.3116, Recall@20: 0.1657, nDCG@20: 0.3502\n",
            "Epoch 95, Train Loss: 0.3176, Precision@20: 0.3133, Recall@20: 0.1680, nDCG@20: 0.3525\n",
            "Epoch 96, Train Loss: 0.3177, Precision@20: 0.3161, Recall@20: 0.1703, nDCG@20: 0.3556\n",
            "Epoch 97, Train Loss: 0.3143, Precision@20: 0.3163, Recall@20: 0.1712, nDCG@20: 0.3567\n",
            "Epoch 98, Train Loss: 0.3140, Precision@20: 0.3193, Recall@20: 0.1725, nDCG@20: 0.3603\n",
            "Epoch 99, Train Loss: 0.3099, Precision@20: 0.3213, Recall@20: 0.1737, nDCG@20: 0.3624\n",
            "Epoch 100, Train Loss: 0.3083, Precision@20: 0.3220, Recall@20: 0.1747, nDCG@20: 0.3636\n",
            "Epoch 101, Train Loss: 0.3048, Precision@20: 0.3243, Recall@20: 0.1746, nDCG@20: 0.3649\n",
            "Epoch 102, Train Loss: 0.3027, Precision@20: 0.3269, Recall@20: 0.1754, nDCG@20: 0.3680\n",
            "Epoch 103, Train Loss: 0.3061, Precision@20: 0.3289, Recall@20: 0.1779, nDCG@20: 0.3701\n",
            "Epoch 104, Train Loss: 0.3010, Precision@20: 0.3312, Recall@20: 0.1795, nDCG@20: 0.3723\n",
            "Epoch 105, Train Loss: 0.3035, Precision@20: 0.3321, Recall@20: 0.1809, nDCG@20: 0.3729\n",
            "Epoch 106, Train Loss: 0.2987, Precision@20: 0.3313, Recall@20: 0.1802, nDCG@20: 0.3720\n",
            "Epoch 107, Train Loss: 0.2945, Precision@20: 0.3325, Recall@20: 0.1806, nDCG@20: 0.3733\n",
            "Epoch 108, Train Loss: 0.2964, Precision@20: 0.3335, Recall@20: 0.1811, nDCG@20: 0.3747\n",
            "Epoch 109, Train Loss: 0.2933, Precision@20: 0.3353, Recall@20: 0.1821, nDCG@20: 0.3768\n",
            "Epoch 110, Train Loss: 0.2910, Precision@20: 0.3378, Recall@20: 0.1831, nDCG@20: 0.3786\n",
            "Epoch 111, Train Loss: 0.2937, Precision@20: 0.3391, Recall@20: 0.1845, nDCG@20: 0.3801\n",
            "Epoch 112, Train Loss: 0.2883, Precision@20: 0.3391, Recall@20: 0.1850, nDCG@20: 0.3800\n",
            "Epoch 113, Train Loss: 0.2887, Precision@20: 0.3385, Recall@20: 0.1849, nDCG@20: 0.3794\n",
            "Epoch 114, Train Loss: 0.2885, Precision@20: 0.3388, Recall@20: 0.1851, nDCG@20: 0.3795\n",
            "Epoch 115, Train Loss: 0.2881, Precision@20: 0.3424, Recall@20: 0.1859, nDCG@20: 0.3830\n",
            "Epoch 116, Train Loss: 0.2825, Precision@20: 0.3442, Recall@20: 0.1867, nDCG@20: 0.3847\n",
            "Epoch 117, Train Loss: 0.2848, Precision@20: 0.3446, Recall@20: 0.1875, nDCG@20: 0.3852\n",
            "Epoch 118, Train Loss: 0.2850, Precision@20: 0.3449, Recall@20: 0.1877, nDCG@20: 0.3848\n",
            "Epoch 119, Train Loss: 0.2802, Precision@20: 0.3458, Recall@20: 0.1887, nDCG@20: 0.3853\n",
            "Epoch 120, Train Loss: 0.2786, Precision@20: 0.3466, Recall@20: 0.1891, nDCG@20: 0.3863\n",
            "Epoch 121, Train Loss: 0.2789, Precision@20: 0.3490, Recall@20: 0.1897, nDCG@20: 0.3896\n",
            "Epoch 122, Train Loss: 0.2813, Precision@20: 0.3506, Recall@20: 0.1907, nDCG@20: 0.3918\n",
            "Epoch 123, Train Loss: 0.2801, Precision@20: 0.3502, Recall@20: 0.1899, nDCG@20: 0.3910\n",
            "Epoch 124, Train Loss: 0.2735, Precision@20: 0.3469, Recall@20: 0.1898, nDCG@20: 0.3870\n",
            "Epoch 125, Train Loss: 0.2761, Precision@20: 0.3469, Recall@20: 0.1896, nDCG@20: 0.3878\n",
            "Epoch 126, Train Loss: 0.2788, Precision@20: 0.3513, Recall@20: 0.1929, nDCG@20: 0.3927\n",
            "Epoch 127, Train Loss: 0.2709, Precision@20: 0.3559, Recall@20: 0.1963, nDCG@20: 0.3976\n",
            "Epoch 128, Train Loss: 0.2757, Precision@20: 0.3578, Recall@20: 0.1977, nDCG@20: 0.3998\n",
            "Epoch 129, Train Loss: 0.2731, Precision@20: 0.3552, Recall@20: 0.1962, nDCG@20: 0.3977\n",
            "Epoch 130, Train Loss: 0.2678, Precision@20: 0.3565, Recall@20: 0.1967, nDCG@20: 0.3990\n",
            "Epoch 131, Train Loss: 0.2693, Precision@20: 0.3551, Recall@20: 0.1952, nDCG@20: 0.3972\n",
            "Epoch 132, Train Loss: 0.2681, Precision@20: 0.3555, Recall@20: 0.1955, nDCG@20: 0.3983\n",
            "Epoch 133, Train Loss: 0.2723, Precision@20: 0.3582, Recall@20: 0.1974, nDCG@20: 0.4017\n",
            "Epoch 134, Train Loss: 0.2668, Precision@20: 0.3615, Recall@20: 0.1990, nDCG@20: 0.4055\n",
            "Epoch 135, Train Loss: 0.2635, Precision@20: 0.3628, Recall@20: 0.1994, nDCG@20: 0.4069\n",
            "Epoch 136, Train Loss: 0.2701, Precision@20: 0.3624, Recall@20: 0.1985, nDCG@20: 0.4061\n",
            "Epoch 137, Train Loss: 0.2614, Precision@20: 0.3634, Recall@20: 0.1992, nDCG@20: 0.4067\n",
            "Epoch 138, Train Loss: 0.2639, Precision@20: 0.3619, Recall@20: 0.1990, nDCG@20: 0.4046\n",
            "Epoch 139, Train Loss: 0.2652, Precision@20: 0.3622, Recall@20: 0.1987, nDCG@20: 0.4045\n",
            "Epoch 140, Train Loss: 0.2667, Precision@20: 0.3603, Recall@20: 0.1974, nDCG@20: 0.4025\n",
            "Epoch 141, Train Loss: 0.2615, Precision@20: 0.3607, Recall@20: 0.1975, nDCG@20: 0.4023\n",
            "Epoch 142, Train Loss: 0.2590, Precision@20: 0.3625, Recall@20: 0.1988, nDCG@20: 0.4047\n",
            "Epoch 143, Train Loss: 0.2569, Precision@20: 0.3664, Recall@20: 0.2009, nDCG@20: 0.4085\n",
            "Epoch 144, Train Loss: 0.2574, Precision@20: 0.3663, Recall@20: 0.2016, nDCG@20: 0.4088\n",
            "Epoch 145, Train Loss: 0.2641, Precision@20: 0.3675, Recall@20: 0.2020, nDCG@20: 0.4093\n",
            "Epoch 146, Train Loss: 0.2567, Precision@20: 0.3676, Recall@20: 0.2022, nDCG@20: 0.4085\n",
            "Epoch 147, Train Loss: 0.2552, Precision@20: 0.3672, Recall@20: 0.2017, nDCG@20: 0.4087\n",
            "Epoch 148, Train Loss: 0.2560, Precision@20: 0.3681, Recall@20: 0.2020, nDCG@20: 0.4102\n",
            "Epoch 149, Train Loss: 0.2561, Precision@20: 0.3691, Recall@20: 0.2026, nDCG@20: 0.4123\n",
            "Epoch 150, Train Loss: 0.2570, Precision@20: 0.3723, Recall@20: 0.2043, nDCG@20: 0.4150\n",
            "Epoch 151, Train Loss: 0.2513, Precision@20: 0.3727, Recall@20: 0.2046, nDCG@20: 0.4153\n",
            "Epoch 152, Train Loss: 0.2503, Precision@20: 0.3708, Recall@20: 0.2035, nDCG@20: 0.4132\n",
            "Epoch 153, Train Loss: 0.2549, Precision@20: 0.3702, Recall@20: 0.2033, nDCG@20: 0.4120\n",
            "Epoch 154, Train Loss: 0.2530, Precision@20: 0.3715, Recall@20: 0.2048, nDCG@20: 0.4135\n",
            "Epoch 155, Train Loss: 0.2530, Precision@20: 0.3743, Recall@20: 0.2070, nDCG@20: 0.4162\n",
            "Epoch 156, Train Loss: 0.2514, Precision@20: 0.3774, Recall@20: 0.2081, nDCG@20: 0.4204\n",
            "Epoch 157, Train Loss: 0.2513, Precision@20: 0.3742, Recall@20: 0.2059, nDCG@20: 0.4169\n",
            "Epoch 158, Train Loss: 0.2495, Precision@20: 0.3682, Recall@20: 0.2026, nDCG@20: 0.4087\n",
            "Epoch 159, Train Loss: 0.2498, Precision@20: 0.3662, Recall@20: 0.2010, nDCG@20: 0.4065\n",
            "Epoch 160, Train Loss: 0.2496, Precision@20: 0.3716, Recall@20: 0.2045, nDCG@20: 0.4129\n",
            "Epoch 161, Train Loss: 0.2469, Precision@20: 0.3770, Recall@20: 0.2084, nDCG@20: 0.4209\n",
            "Epoch 162, Train Loss: 0.2483, Precision@20: 0.3794, Recall@20: 0.2104, nDCG@20: 0.4245\n",
            "Epoch 163, Train Loss: 0.2473, Precision@20: 0.3802, Recall@20: 0.2106, nDCG@20: 0.4256\n",
            "Epoch 164, Train Loss: 0.2485, Precision@20: 0.3778, Recall@20: 0.2080, nDCG@20: 0.4222\n",
            "Epoch 165, Train Loss: 0.2475, Precision@20: 0.3738, Recall@20: 0.2044, nDCG@20: 0.4153\n",
            "Epoch 166, Train Loss: 0.2475, Precision@20: 0.3718, Recall@20: 0.2032, nDCG@20: 0.4133\n",
            "Epoch 167, Train Loss: 0.2453, Precision@20: 0.3744, Recall@20: 0.2049, nDCG@20: 0.4168\n",
            "Epoch 168, Train Loss: 0.2485, Precision@20: 0.3824, Recall@20: 0.2110, nDCG@20: 0.4261\n",
            "Epoch 169, Train Loss: 0.2451, Precision@20: 0.3838, Recall@20: 0.2129, nDCG@20: 0.4283\n",
            "Epoch 170, Train Loss: 0.2414, Precision@20: 0.3810, Recall@20: 0.2110, nDCG@20: 0.4252\n",
            "Epoch 171, Train Loss: 0.2438, Precision@20: 0.3779, Recall@20: 0.2083, nDCG@20: 0.4212\n",
            "Epoch 172, Train Loss: 0.2404, Precision@20: 0.3740, Recall@20: 0.2048, nDCG@20: 0.4168\n",
            "Epoch 173, Train Loss: 0.2424, Precision@20: 0.3743, Recall@20: 0.2059, nDCG@20: 0.4164\n",
            "Epoch 174, Train Loss: 0.2389, Precision@20: 0.3730, Recall@20: 0.2048, nDCG@20: 0.4144\n",
            "Epoch 175, Train Loss: 0.2428, Precision@20: 0.3793, Recall@20: 0.2092, nDCG@20: 0.4210\n",
            "Epoch 176, Train Loss: 0.2437, Precision@20: 0.3836, Recall@20: 0.2120, nDCG@20: 0.4280\n",
            "Epoch 177, Train Loss: 0.2442, Precision@20: 0.3859, Recall@20: 0.2132, nDCG@20: 0.4312\n",
            "Epoch 178, Train Loss: 0.2391, Precision@20: 0.3845, Recall@20: 0.2123, nDCG@20: 0.4303\n",
            "Epoch 179, Train Loss: 0.2411, Precision@20: 0.3815, Recall@20: 0.2101, nDCG@20: 0.4267\n",
            "Epoch 180, Train Loss: 0.2435, Precision@20: 0.3801, Recall@20: 0.2094, nDCG@20: 0.4249\n",
            "Epoch 181, Train Loss: 0.2398, Precision@20: 0.3818, Recall@20: 0.2101, nDCG@20: 0.4276\n",
            "Epoch 182, Train Loss: 0.2371, Precision@20: 0.3862, Recall@20: 0.2132, nDCG@20: 0.4321\n",
            "Epoch 183, Train Loss: 0.2366, Precision@20: 0.3883, Recall@20: 0.2144, nDCG@20: 0.4341\n",
            "Epoch 184, Train Loss: 0.2387, Precision@20: 0.3879, Recall@20: 0.2143, nDCG@20: 0.4337\n",
            "Epoch 185, Train Loss: 0.2377, Precision@20: 0.3867, Recall@20: 0.2123, nDCG@20: 0.4320\n",
            "Epoch 186, Train Loss: 0.2343, Precision@20: 0.3849, Recall@20: 0.2119, nDCG@20: 0.4300\n",
            "Epoch 187, Train Loss: 0.2400, Precision@20: 0.3874, Recall@20: 0.2138, nDCG@20: 0.4324\n",
            "Epoch 188, Train Loss: 0.2370, Precision@20: 0.3907, Recall@20: 0.2170, nDCG@20: 0.4357\n",
            "Epoch 189, Train Loss: 0.2372, Precision@20: 0.3920, Recall@20: 0.2178, nDCG@20: 0.4369\n",
            "Epoch 190, Train Loss: 0.2355, Precision@20: 0.3923, Recall@20: 0.2183, nDCG@20: 0.4375\n",
            "Epoch 191, Train Loss: 0.2341, Precision@20: 0.3899, Recall@20: 0.2163, nDCG@20: 0.4355\n",
            "Epoch 192, Train Loss: 0.2356, Precision@20: 0.3847, Recall@20: 0.2128, nDCG@20: 0.4309\n",
            "Epoch 193, Train Loss: 0.2333, Precision@20: 0.3849, Recall@20: 0.2125, nDCG@20: 0.4301\n",
            "Epoch 194, Train Loss: 0.2347, Precision@20: 0.3901, Recall@20: 0.2164, nDCG@20: 0.4371\n",
            "Epoch 195, Train Loss: 0.2343, Precision@20: 0.3971, Recall@20: 0.2207, nDCG@20: 0.4437\n",
            "Epoch 196, Train Loss: 0.2349, Precision@20: 0.3995, Recall@20: 0.2213, nDCG@20: 0.4462\n",
            "Epoch 197, Train Loss: 0.2337, Precision@20: 0.4002, Recall@20: 0.2218, nDCG@20: 0.4469\n",
            "Epoch 198, Train Loss: 0.2339, Precision@20: 0.3955, Recall@20: 0.2191, nDCG@20: 0.4419\n",
            "Epoch 199, Train Loss: 0.2347, Precision@20: 0.3915, Recall@20: 0.2158, nDCG@20: 0.4365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "k=3 hops"
      ],
      "metadata": {
        "id": "Gdt0RJacgQbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "from torch_geometric.nn import SAGEConv, to_hetero\n",
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, num_layers=3, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        # Input to hidden layer\n",
        "        self.convs.append(SAGEConv((-1, -1), hidden_channels))\n",
        "\n",
        "        # Hidden to hidden layers\n",
        "        for _ in range(1, num_layers - 1):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "\n",
        "        # Hidden to output layer\n",
        "        self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "            x = self.dropout(x)  # Apply dropout after each hidden layer\n",
        "\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, data: HeteroData, hidden_channels):\n",
        "        super().__init__()\n",
        "        # Extract the number of nodes for users and movies\n",
        "        num_users = data['user'].num_nodes\n",
        "        num_movies = data['movie'].num_nodes\n",
        "        user_feat_dim = data['user'].x.size(1)\n",
        "        movie_feat_dim = data['movie'].x.size(1)\n",
        "\n",
        "        # Learnable embeddings for users and movies\n",
        "        self.user_emb = torch.nn.Embedding(num_users, hidden_channels)\n",
        "        self.movie_emb = torch.nn.Embedding(num_movies, hidden_channels)\n",
        "\n",
        "        # Project user and movie features to the hidden dimension\n",
        "        self.user_lin = torch.nn.Linear(user_feat_dim, hidden_channels)\n",
        "        self.movie_lin = torch.nn.Linear(movie_feat_dim, hidden_channels)\n",
        "\n",
        "        self.gnn = GNN(hidden_channels)\n",
        "\n",
        "        self.gnn = to_hetero(self.gnn, metadata=data.metadata())\n",
        "\n",
        "    def forward(self, data: HeteroData) -> dict:\n",
        "        # Generate initial embeddings for users and movies\n",
        "        x_dict = {\n",
        "            \"user\": self.user_lin(data[\"user\"].x) + self.user_emb.weight,  # Combine user features and embeddings\n",
        "            \"movie\": self.movie_lin(data[\"movie\"].x) + self.movie_emb.weight,  # Combine movie features and embeddings\n",
        "        }\n",
        "\n",
        "        # Apply the heterogeneous GNN\n",
        "        x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
        "        return x_dict\n",
        "\n",
        "\n",
        "def bpr_loss(user_emb, pos_movie_emb, neg_movie_emb):\n",
        "    pos_scores = (user_emb * pos_movie_emb).sum(dim=-1)\n",
        "    neg_scores = (user_emb * neg_movie_emb).sum(dim=-1)\n",
        "    loss = -torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def train_bpr(data: HeteroData, model, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    x_dict = model(data)\n",
        "    user_emb = x_dict['user']\n",
        "    movie_emb = x_dict['movie']\n",
        "\n",
        "    # Positive and negative edge sampling\n",
        "    pos_edge_index = data['user', 'rates', 'movie'].edge_index\n",
        "    neg_edge_index = torch.randint(\n",
        "        low=0, high=data['movie'].num_nodes, size=pos_edge_index.size(), device=pos_edge_index.device\n",
        "    )\n",
        "\n",
        "    # Extract embeddings for positive and negative edges\n",
        "    user_emb = user_emb[pos_edge_index[0]]\n",
        "    pos_movie_emb = movie_emb[pos_edge_index[1]]\n",
        "    neg_movie_emb = movie_emb[neg_edge_index[1]]\n",
        "\n",
        "    # Compute BPR loss\n",
        "    loss = bpr_loss(user_emb, pos_movie_emb, neg_movie_emb)\n",
        "    loss.backward()\n",
        "    #print(\"User Embedding Gradients:\", model.user_emb.weight.grad)\n",
        "    #print(\"Movie Embedding Gradients:\", model.movie_emb.weight.grad)\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(data: HeteroData, model, k=20):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_dict = model(data)\n",
        "        user_emb = x_dict['user']\n",
        "        movie_emb = x_dict['movie']\n",
        "\n",
        "        # Extract edge indices for positive edges\n",
        "        pos_edge_index = data['user', 'rates', 'movie'].edge_index\n",
        "\n",
        "        # Compute scores for all movies for each user\n",
        "        scores = torch.matmul(user_emb, movie_emb.t())  # [num_users, num_movies]\n",
        "\n",
        "        # Extract ground-truth positive edges\n",
        "        user_ids = pos_edge_index[0]  # Users in positive edges\n",
        "        movie_ids = pos_edge_index[1]  # Movies in positive edges\n",
        "\n",
        "        # Initialize metrics\n",
        "        precision = 0.0\n",
        "        recall = 0.0\n",
        "        ndcg = 0.0\n",
        "\n",
        "        for user_id in user_ids.unique():\n",
        "            user_scores = scores[user_id]  # Scores for the current user [num_movies]\n",
        "            true_movie_ids = movie_ids[user_ids == user_id]  # Ground truth movies for the user\n",
        "\n",
        "            # Skip users with no positive edges\n",
        "            if true_movie_ids.numel() == 0:\n",
        "                continue\n",
        "\n",
        "            # Get top-K movie indices\n",
        "            _, top_k_indices = torch.topk(user_scores, k)\n",
        "\n",
        "            # Compute Precision@K\n",
        "            hits = torch.isin(top_k_indices, true_movie_ids).sum().item()\n",
        "            precision += hits / k\n",
        "\n",
        "            # Compute Recall@K\n",
        "            recall += hits / true_movie_ids.numel()\n",
        "\n",
        "            # Compute nDCG@K\n",
        "            gains = torch.isin(top_k_indices, true_movie_ids).float()\n",
        "            discounts = torch.log2(torch.arange(2, k + 2).float())\n",
        "            dcg = (gains / discounts).sum()\n",
        "            idcg = (1.0 / discounts[: min(k, true_movie_ids.numel())]).sum()\n",
        "            ndcg += dcg / idcg\n",
        "\n",
        "        # Average metrics over all users\n",
        "        num_users = user_ids.unique().numel()\n",
        "        precision /= num_users\n",
        "        recall /= num_users\n",
        "        ndcg /= num_users\n",
        "\n",
        "    return precision, recall, ndcg\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "hidden_channels = 64\n",
        "model = Model(data, hidden_channels=hidden_channels)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_bpr(train_data, model, optimizer)\n",
        "    precision, recall, ndcg = evaluate(val_data, model, k=20)\n",
        "\n",
        "    print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Precision@20: {precision:.4f}, Recall@20: {recall:.4f}, nDCG@20: {ndcg:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpN3hg3XebCS",
        "outputId": "4ce1e223-d8fb-41b5-9d63-dfee337a6a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Train Loss: 0.6951, Precision@20: 0.0159, Recall@20: 0.0070, nDCG@20: 0.0132\n",
            "Epoch 1, Train Loss: 0.6771, Precision@20: 0.0273, Recall@20: 0.0115, nDCG@20: 0.0227\n",
            "Epoch 2, Train Loss: 0.6569, Precision@20: 0.0356, Recall@20: 0.0150, nDCG@20: 0.0291\n",
            "Epoch 3, Train Loss: 0.6456, Precision@20: 0.0477, Recall@20: 0.0198, nDCG@20: 0.0390\n",
            "Epoch 4, Train Loss: 0.6249, Precision@20: 0.0647, Recall@20: 0.0261, nDCG@20: 0.0544\n",
            "Epoch 5, Train Loss: 0.6050, Precision@20: 0.0733, Recall@20: 0.0279, nDCG@20: 0.0644\n",
            "Epoch 6, Train Loss: 0.5799, Precision@20: 0.0843, Recall@20: 0.0308, nDCG@20: 0.0753\n",
            "Epoch 7, Train Loss: 0.5648, Precision@20: 0.0964, Recall@20: 0.0349, nDCG@20: 0.0857\n",
            "Epoch 8, Train Loss: 0.5499, Precision@20: 0.1092, Recall@20: 0.0396, nDCG@20: 0.0963\n",
            "Epoch 9, Train Loss: 0.5374, Precision@20: 0.1168, Recall@20: 0.0426, nDCG@20: 0.1052\n",
            "Epoch 10, Train Loss: 0.5223, Precision@20: 0.1227, Recall@20: 0.0459, nDCG@20: 0.1167\n",
            "Epoch 11, Train Loss: 0.5191, Precision@20: 0.1340, Recall@20: 0.0495, nDCG@20: 0.1302\n",
            "Epoch 12, Train Loss: 0.5097, Precision@20: 0.1467, Recall@20: 0.0547, nDCG@20: 0.1424\n",
            "Epoch 13, Train Loss: 0.4936, Precision@20: 0.1602, Recall@20: 0.0619, nDCG@20: 0.1535\n",
            "Epoch 14, Train Loss: 0.4881, Precision@20: 0.1706, Recall@20: 0.0688, nDCG@20: 0.1670\n",
            "Epoch 15, Train Loss: 0.4868, Precision@20: 0.1787, Recall@20: 0.0724, nDCG@20: 0.1752\n",
            "Epoch 16, Train Loss: 0.4735, Precision@20: 0.1863, Recall@20: 0.0758, nDCG@20: 0.1806\n",
            "Epoch 17, Train Loss: 0.4721, Precision@20: 0.1925, Recall@20: 0.0806, nDCG@20: 0.1853\n",
            "Epoch 18, Train Loss: 0.4642, Precision@20: 0.1884, Recall@20: 0.0813, nDCG@20: 0.1829\n",
            "Epoch 19, Train Loss: 0.4556, Precision@20: 0.1896, Recall@20: 0.0828, nDCG@20: 0.1872\n",
            "Epoch 20, Train Loss: 0.4567, Precision@20: 0.1950, Recall@20: 0.0865, nDCG@20: 0.1937\n",
            "Epoch 21, Train Loss: 0.4552, Precision@20: 0.1990, Recall@20: 0.0888, nDCG@20: 0.1996\n",
            "Epoch 22, Train Loss: 0.4451, Precision@20: 0.2041, Recall@20: 0.0913, nDCG@20: 0.2059\n",
            "Epoch 23, Train Loss: 0.4388, Precision@20: 0.2101, Recall@20: 0.0964, nDCG@20: 0.2147\n",
            "Epoch 24, Train Loss: 0.4380, Precision@20: 0.2178, Recall@20: 0.1025, nDCG@20: 0.2233\n",
            "Epoch 25, Train Loss: 0.4315, Precision@20: 0.2251, Recall@20: 0.1100, nDCG@20: 0.2312\n",
            "Epoch 26, Train Loss: 0.4286, Precision@20: 0.2311, Recall@20: 0.1153, nDCG@20: 0.2391\n",
            "Epoch 27, Train Loss: 0.4263, Precision@20: 0.2347, Recall@20: 0.1198, nDCG@20: 0.2526\n",
            "Epoch 28, Train Loss: 0.4279, Precision@20: 0.2367, Recall@20: 0.1214, nDCG@20: 0.2647\n",
            "Epoch 29, Train Loss: 0.4282, Precision@20: 0.2390, Recall@20: 0.1228, nDCG@20: 0.2687\n",
            "Epoch 30, Train Loss: 0.4272, Precision@20: 0.2426, Recall@20: 0.1243, nDCG@20: 0.2721\n",
            "Epoch 31, Train Loss: 0.4193, Precision@20: 0.2454, Recall@20: 0.1249, nDCG@20: 0.2746\n",
            "Epoch 32, Train Loss: 0.4197, Precision@20: 0.2452, Recall@20: 0.1236, nDCG@20: 0.2741\n",
            "Epoch 33, Train Loss: 0.4186, Precision@20: 0.2441, Recall@20: 0.1225, nDCG@20: 0.2728\n",
            "Epoch 34, Train Loss: 0.4175, Precision@20: 0.2420, Recall@20: 0.1199, nDCG@20: 0.2706\n",
            "Epoch 35, Train Loss: 0.4123, Precision@20: 0.2406, Recall@20: 0.1183, nDCG@20: 0.2687\n",
            "Epoch 36, Train Loss: 0.4096, Precision@20: 0.2400, Recall@20: 0.1169, nDCG@20: 0.2679\n",
            "Epoch 37, Train Loss: 0.4096, Precision@20: 0.2405, Recall@20: 0.1171, nDCG@20: 0.2683\n",
            "Epoch 38, Train Loss: 0.4041, Precision@20: 0.2425, Recall@20: 0.1184, nDCG@20: 0.2700\n",
            "Epoch 39, Train Loss: 0.4065, Precision@20: 0.2447, Recall@20: 0.1197, nDCG@20: 0.2718\n",
            "Epoch 40, Train Loss: 0.4038, Precision@20: 0.2450, Recall@20: 0.1196, nDCG@20: 0.2726\n",
            "Epoch 41, Train Loss: 0.3985, Precision@20: 0.2466, Recall@20: 0.1198, nDCG@20: 0.2739\n",
            "Epoch 42, Train Loss: 0.4003, Precision@20: 0.2481, Recall@20: 0.1203, nDCG@20: 0.2754\n",
            "Epoch 43, Train Loss: 0.4032, Precision@20: 0.2479, Recall@20: 0.1203, nDCG@20: 0.2753\n",
            "Epoch 44, Train Loss: 0.4001, Precision@20: 0.2484, Recall@20: 0.1201, nDCG@20: 0.2757\n",
            "Epoch 45, Train Loss: 0.3984, Precision@20: 0.2486, Recall@20: 0.1210, nDCG@20: 0.2762\n",
            "Epoch 46, Train Loss: 0.3955, Precision@20: 0.2498, Recall@20: 0.1220, nDCG@20: 0.2775\n",
            "Epoch 47, Train Loss: 0.3950, Precision@20: 0.2501, Recall@20: 0.1230, nDCG@20: 0.2776\n",
            "Epoch 48, Train Loss: 0.3952, Precision@20: 0.2500, Recall@20: 0.1234, nDCG@20: 0.2776\n",
            "Epoch 49, Train Loss: 0.3898, Precision@20: 0.2508, Recall@20: 0.1233, nDCG@20: 0.2780\n",
            "Epoch 50, Train Loss: 0.3897, Precision@20: 0.2502, Recall@20: 0.1228, nDCG@20: 0.2777\n",
            "Epoch 51, Train Loss: 0.3895, Precision@20: 0.2522, Recall@20: 0.1240, nDCG@20: 0.2798\n",
            "Epoch 52, Train Loss: 0.3907, Precision@20: 0.2532, Recall@20: 0.1247, nDCG@20: 0.2819\n",
            "Epoch 53, Train Loss: 0.3902, Precision@20: 0.2536, Recall@20: 0.1246, nDCG@20: 0.2826\n",
            "Epoch 54, Train Loss: 0.3862, Precision@20: 0.2542, Recall@20: 0.1248, nDCG@20: 0.2835\n",
            "Epoch 55, Train Loss: 0.3848, Precision@20: 0.2556, Recall@20: 0.1255, nDCG@20: 0.2844\n",
            "Epoch 56, Train Loss: 0.3836, Precision@20: 0.2554, Recall@20: 0.1251, nDCG@20: 0.2841\n",
            "Epoch 57, Train Loss: 0.3828, Precision@20: 0.2549, Recall@20: 0.1254, nDCG@20: 0.2841\n",
            "Epoch 58, Train Loss: 0.3822, Precision@20: 0.2565, Recall@20: 0.1266, nDCG@20: 0.2854\n",
            "Epoch 59, Train Loss: 0.3804, Precision@20: 0.2581, Recall@20: 0.1285, nDCG@20: 0.2866\n",
            "Epoch 60, Train Loss: 0.3744, Precision@20: 0.2579, Recall@20: 0.1292, nDCG@20: 0.2869\n",
            "Epoch 61, Train Loss: 0.3763, Precision@20: 0.2576, Recall@20: 0.1300, nDCG@20: 0.2869\n",
            "Epoch 62, Train Loss: 0.3783, Precision@20: 0.2591, Recall@20: 0.1301, nDCG@20: 0.2881\n",
            "Epoch 63, Train Loss: 0.3759, Precision@20: 0.2599, Recall@20: 0.1311, nDCG@20: 0.2888\n",
            "Epoch 64, Train Loss: 0.3708, Precision@20: 0.2589, Recall@20: 0.1310, nDCG@20: 0.2882\n",
            "Epoch 65, Train Loss: 0.3725, Precision@20: 0.2601, Recall@20: 0.1325, nDCG@20: 0.2893\n",
            "Epoch 66, Train Loss: 0.3766, Precision@20: 0.2604, Recall@20: 0.1326, nDCG@20: 0.2904\n",
            "Epoch 67, Train Loss: 0.3724, Precision@20: 0.2617, Recall@20: 0.1343, nDCG@20: 0.2920\n",
            "Epoch 68, Train Loss: 0.3730, Precision@20: 0.2615, Recall@20: 0.1343, nDCG@20: 0.2925\n",
            "Epoch 69, Train Loss: 0.3704, Precision@20: 0.2619, Recall@20: 0.1340, nDCG@20: 0.2926\n",
            "Epoch 70, Train Loss: 0.3670, Precision@20: 0.2626, Recall@20: 0.1333, nDCG@20: 0.2928\n",
            "Epoch 71, Train Loss: 0.3693, Precision@20: 0.2628, Recall@20: 0.1326, nDCG@20: 0.2935\n",
            "Epoch 72, Train Loss: 0.3666, Precision@20: 0.2627, Recall@20: 0.1322, nDCG@20: 0.2932\n",
            "Epoch 73, Train Loss: 0.3665, Precision@20: 0.2651, Recall@20: 0.1345, nDCG@20: 0.2956\n",
            "Epoch 74, Train Loss: 0.3623, Precision@20: 0.2647, Recall@20: 0.1355, nDCG@20: 0.2958\n",
            "Epoch 75, Train Loss: 0.3624, Precision@20: 0.2645, Recall@20: 0.1360, nDCG@20: 0.2957\n",
            "Epoch 76, Train Loss: 0.3585, Precision@20: 0.2655, Recall@20: 0.1365, nDCG@20: 0.2971\n",
            "Epoch 77, Train Loss: 0.3610, Precision@20: 0.2661, Recall@20: 0.1372, nDCG@20: 0.2981\n",
            "Epoch 78, Train Loss: 0.3592, Precision@20: 0.2656, Recall@20: 0.1374, nDCG@20: 0.2981\n",
            "Epoch 79, Train Loss: 0.3574, Precision@20: 0.2659, Recall@20: 0.1378, nDCG@20: 0.2985\n",
            "Epoch 80, Train Loss: 0.3579, Precision@20: 0.2671, Recall@20: 0.1380, nDCG@20: 0.3000\n",
            "Epoch 81, Train Loss: 0.3544, Precision@20: 0.2686, Recall@20: 0.1381, nDCG@20: 0.3016\n",
            "Epoch 82, Train Loss: 0.3533, Precision@20: 0.2689, Recall@20: 0.1386, nDCG@20: 0.3017\n",
            "Epoch 83, Train Loss: 0.3505, Precision@20: 0.2703, Recall@20: 0.1397, nDCG@20: 0.3031\n",
            "Epoch 84, Train Loss: 0.3478, Precision@20: 0.2714, Recall@20: 0.1410, nDCG@20: 0.3041\n",
            "Epoch 85, Train Loss: 0.3481, Precision@20: 0.2733, Recall@20: 0.1418, nDCG@20: 0.3068\n",
            "Epoch 86, Train Loss: 0.3468, Precision@20: 0.2765, Recall@20: 0.1427, nDCG@20: 0.3101\n",
            "Epoch 87, Train Loss: 0.3491, Precision@20: 0.2780, Recall@20: 0.1430, nDCG@20: 0.3124\n",
            "Epoch 88, Train Loss: 0.3428, Precision@20: 0.2818, Recall@20: 0.1452, nDCG@20: 0.3160\n",
            "Epoch 89, Train Loss: 0.3450, Precision@20: 0.2840, Recall@20: 0.1465, nDCG@20: 0.3193\n",
            "Epoch 90, Train Loss: 0.3409, Precision@20: 0.2832, Recall@20: 0.1458, nDCG@20: 0.3194\n",
            "Epoch 91, Train Loss: 0.3400, Precision@20: 0.2847, Recall@20: 0.1476, nDCG@20: 0.3211\n",
            "Epoch 92, Train Loss: 0.3396, Precision@20: 0.2860, Recall@20: 0.1484, nDCG@20: 0.3228\n",
            "Epoch 93, Train Loss: 0.3388, Precision@20: 0.2872, Recall@20: 0.1487, nDCG@20: 0.3242\n",
            "Epoch 94, Train Loss: 0.3378, Precision@20: 0.2898, Recall@20: 0.1500, nDCG@20: 0.3268\n",
            "Epoch 95, Train Loss: 0.3332, Precision@20: 0.2922, Recall@20: 0.1524, nDCG@20: 0.3312\n",
            "Epoch 96, Train Loss: 0.3302, Precision@20: 0.2938, Recall@20: 0.1538, nDCG@20: 0.3336\n",
            "Epoch 97, Train Loss: 0.3239, Precision@20: 0.2940, Recall@20: 0.1546, nDCG@20: 0.3345\n",
            "Epoch 98, Train Loss: 0.3245, Precision@20: 0.2941, Recall@20: 0.1552, nDCG@20: 0.3353\n",
            "Epoch 99, Train Loss: 0.3264, Precision@20: 0.2952, Recall@20: 0.1556, nDCG@20: 0.3363\n",
            "Epoch 100, Train Loss: 0.3256, Precision@20: 0.2982, Recall@20: 0.1585, nDCG@20: 0.3393\n",
            "Epoch 101, Train Loss: 0.3275, Precision@20: 0.3000, Recall@20: 0.1589, nDCG@20: 0.3418\n",
            "Epoch 102, Train Loss: 0.3229, Precision@20: 0.3037, Recall@20: 0.1599, nDCG@20: 0.3448\n",
            "Epoch 103, Train Loss: 0.3210, Precision@20: 0.3042, Recall@20: 0.1605, nDCG@20: 0.3461\n",
            "Epoch 104, Train Loss: 0.3170, Precision@20: 0.3049, Recall@20: 0.1608, nDCG@20: 0.3470\n",
            "Epoch 105, Train Loss: 0.3199, Precision@20: 0.3056, Recall@20: 0.1615, nDCG@20: 0.3487\n",
            "Epoch 106, Train Loss: 0.3192, Precision@20: 0.3033, Recall@20: 0.1623, nDCG@20: 0.3471\n",
            "Epoch 107, Train Loss: 0.3135, Precision@20: 0.3023, Recall@20: 0.1629, nDCG@20: 0.3468\n",
            "Epoch 108, Train Loss: 0.3153, Precision@20: 0.3096, Recall@20: 0.1654, nDCG@20: 0.3537\n",
            "Epoch 109, Train Loss: 0.3125, Precision@20: 0.3148, Recall@20: 0.1668, nDCG@20: 0.3584\n",
            "Epoch 110, Train Loss: 0.3126, Precision@20: 0.3165, Recall@20: 0.1678, nDCG@20: 0.3606\n",
            "Epoch 111, Train Loss: 0.3082, Precision@20: 0.3177, Recall@20: 0.1699, nDCG@20: 0.3606\n",
            "Epoch 112, Train Loss: 0.3087, Precision@20: 0.3188, Recall@20: 0.1704, nDCG@20: 0.3615\n",
            "Epoch 113, Train Loss: 0.3041, Precision@20: 0.3203, Recall@20: 0.1703, nDCG@20: 0.3629\n",
            "Epoch 114, Train Loss: 0.3028, Precision@20: 0.3208, Recall@20: 0.1721, nDCG@20: 0.3641\n",
            "Epoch 115, Train Loss: 0.3032, Precision@20: 0.3189, Recall@20: 0.1718, nDCG@20: 0.3623\n",
            "Epoch 116, Train Loss: 0.3043, Precision@20: 0.3258, Recall@20: 0.1739, nDCG@20: 0.3704\n",
            "Epoch 117, Train Loss: 0.3016, Precision@20: 0.3302, Recall@20: 0.1757, nDCG@20: 0.3764\n",
            "Epoch 118, Train Loss: 0.3022, Precision@20: 0.3317, Recall@20: 0.1765, nDCG@20: 0.3770\n",
            "Epoch 119, Train Loss: 0.3030, Precision@20: 0.3262, Recall@20: 0.1758, nDCG@20: 0.3697\n",
            "Epoch 120, Train Loss: 0.2940, Precision@20: 0.3238, Recall@20: 0.1757, nDCG@20: 0.3671\n",
            "Epoch 121, Train Loss: 0.2946, Precision@20: 0.3278, Recall@20: 0.1778, nDCG@20: 0.3718\n",
            "Epoch 122, Train Loss: 0.2925, Precision@20: 0.3314, Recall@20: 0.1790, nDCG@20: 0.3763\n",
            "Epoch 123, Train Loss: 0.2909, Precision@20: 0.3336, Recall@20: 0.1795, nDCG@20: 0.3788\n",
            "Epoch 124, Train Loss: 0.2906, Precision@20: 0.3349, Recall@20: 0.1805, nDCG@20: 0.3793\n",
            "Epoch 125, Train Loss: 0.2915, Precision@20: 0.3330, Recall@20: 0.1800, nDCG@20: 0.3772\n",
            "Epoch 126, Train Loss: 0.2919, Precision@20: 0.3308, Recall@20: 0.1788, nDCG@20: 0.3736\n",
            "Epoch 127, Train Loss: 0.2876, Precision@20: 0.3339, Recall@20: 0.1795, nDCG@20: 0.3773\n",
            "Epoch 128, Train Loss: 0.2905, Precision@20: 0.3377, Recall@20: 0.1813, nDCG@20: 0.3818\n",
            "Epoch 129, Train Loss: 0.2854, Precision@20: 0.3396, Recall@20: 0.1822, nDCG@20: 0.3841\n",
            "Epoch 130, Train Loss: 0.2851, Precision@20: 0.3405, Recall@20: 0.1820, nDCG@20: 0.3853\n",
            "Epoch 131, Train Loss: 0.2834, Precision@20: 0.3397, Recall@20: 0.1811, nDCG@20: 0.3820\n",
            "Epoch 132, Train Loss: 0.2869, Precision@20: 0.3403, Recall@20: 0.1824, nDCG@20: 0.3815\n",
            "Epoch 133, Train Loss: 0.2827, Precision@20: 0.3390, Recall@20: 0.1814, nDCG@20: 0.3795\n",
            "Epoch 134, Train Loss: 0.2830, Precision@20: 0.3447, Recall@20: 0.1836, nDCG@20: 0.3853\n",
            "Epoch 135, Train Loss: 0.2823, Precision@20: 0.3462, Recall@20: 0.1844, nDCG@20: 0.3876\n",
            "Epoch 136, Train Loss: 0.2824, Precision@20: 0.3408, Recall@20: 0.1823, nDCG@20: 0.3823\n",
            "Epoch 137, Train Loss: 0.2776, Precision@20: 0.3365, Recall@20: 0.1811, nDCG@20: 0.3778\n",
            "Epoch 138, Train Loss: 0.2813, Precision@20: 0.3415, Recall@20: 0.1833, nDCG@20: 0.3844\n",
            "Epoch 139, Train Loss: 0.2755, Precision@20: 0.3482, Recall@20: 0.1865, nDCG@20: 0.3919\n",
            "Epoch 140, Train Loss: 0.2776, Precision@20: 0.3458, Recall@20: 0.1851, nDCG@20: 0.3895\n",
            "Epoch 141, Train Loss: 0.2795, Precision@20: 0.3420, Recall@20: 0.1840, nDCG@20: 0.3836\n",
            "Epoch 142, Train Loss: 0.2750, Precision@20: 0.3411, Recall@20: 0.1822, nDCG@20: 0.3823\n",
            "Epoch 143, Train Loss: 0.2745, Precision@20: 0.3379, Recall@20: 0.1807, nDCG@20: 0.3797\n",
            "Epoch 144, Train Loss: 0.2719, Precision@20: 0.3472, Recall@20: 0.1864, nDCG@20: 0.3906\n",
            "Epoch 145, Train Loss: 0.2785, Precision@20: 0.3511, Recall@20: 0.1895, nDCG@20: 0.3959\n",
            "Epoch 146, Train Loss: 0.2678, Precision@20: 0.3456, Recall@20: 0.1847, nDCG@20: 0.3893\n",
            "Epoch 147, Train Loss: 0.2703, Precision@20: 0.3406, Recall@20: 0.1820, nDCG@20: 0.3838\n",
            "Epoch 148, Train Loss: 0.2737, Precision@20: 0.3499, Recall@20: 0.1877, nDCG@20: 0.3935\n",
            "Epoch 149, Train Loss: 0.2694, Precision@20: 0.3552, Recall@20: 0.1912, nDCG@20: 0.4005\n",
            "Epoch 150, Train Loss: 0.2720, Precision@20: 0.3523, Recall@20: 0.1883, nDCG@20: 0.3963\n",
            "Epoch 151, Train Loss: 0.2672, Precision@20: 0.3483, Recall@20: 0.1856, nDCG@20: 0.3906\n",
            "Epoch 152, Train Loss: 0.2711, Precision@20: 0.3494, Recall@20: 0.1861, nDCG@20: 0.3910\n",
            "Epoch 153, Train Loss: 0.2683, Precision@20: 0.3514, Recall@20: 0.1884, nDCG@20: 0.3939\n",
            "Epoch 154, Train Loss: 0.2664, Precision@20: 0.3518, Recall@20: 0.1889, nDCG@20: 0.3948\n",
            "Epoch 155, Train Loss: 0.2662, Precision@20: 0.3522, Recall@20: 0.1893, nDCG@20: 0.3954\n",
            "Epoch 156, Train Loss: 0.2691, Precision@20: 0.3517, Recall@20: 0.1881, nDCG@20: 0.3934\n",
            "Epoch 157, Train Loss: 0.2640, Precision@20: 0.3536, Recall@20: 0.1892, nDCG@20: 0.3943\n",
            "Epoch 158, Train Loss: 0.2658, Precision@20: 0.3523, Recall@20: 0.1877, nDCG@20: 0.3924\n",
            "Epoch 159, Train Loss: 0.2675, Precision@20: 0.3595, Recall@20: 0.1932, nDCG@20: 0.4006\n",
            "Epoch 160, Train Loss: 0.2662, Precision@20: 0.3553, Recall@20: 0.1913, nDCG@20: 0.3980\n",
            "Epoch 161, Train Loss: 0.2608, Precision@20: 0.3537, Recall@20: 0.1887, nDCG@20: 0.3952\n",
            "Epoch 162, Train Loss: 0.2646, Precision@20: 0.3562, Recall@20: 0.1904, nDCG@20: 0.3976\n",
            "Epoch 163, Train Loss: 0.2631, Precision@20: 0.3583, Recall@20: 0.1923, nDCG@20: 0.4006\n",
            "Epoch 164, Train Loss: 0.2584, Precision@20: 0.3589, Recall@20: 0.1924, nDCG@20: 0.4012\n",
            "Epoch 165, Train Loss: 0.2591, Precision@20: 0.3528, Recall@20: 0.1891, nDCG@20: 0.3947\n",
            "Epoch 166, Train Loss: 0.2607, Precision@20: 0.3575, Recall@20: 0.1916, nDCG@20: 0.3999\n",
            "Epoch 167, Train Loss: 0.2600, Precision@20: 0.3629, Recall@20: 0.1946, nDCG@20: 0.4070\n",
            "Epoch 168, Train Loss: 0.2586, Precision@20: 0.3633, Recall@20: 0.1940, nDCG@20: 0.4065\n",
            "Epoch 169, Train Loss: 0.2582, Precision@20: 0.3583, Recall@20: 0.1908, nDCG@20: 0.3998\n",
            "Epoch 170, Train Loss: 0.2557, Precision@20: 0.3577, Recall@20: 0.1909, nDCG@20: 0.3986\n",
            "Epoch 171, Train Loss: 0.2541, Precision@20: 0.3603, Recall@20: 0.1933, nDCG@20: 0.4021\n",
            "Epoch 172, Train Loss: 0.2562, Precision@20: 0.3638, Recall@20: 0.1964, nDCG@20: 0.4068\n",
            "Epoch 173, Train Loss: 0.2589, Precision@20: 0.3645, Recall@20: 0.1979, nDCG@20: 0.4084\n",
            "Epoch 174, Train Loss: 0.2536, Precision@20: 0.3630, Recall@20: 0.1962, nDCG@20: 0.4061\n",
            "Epoch 175, Train Loss: 0.2494, Precision@20: 0.3632, Recall@20: 0.1963, nDCG@20: 0.4057\n",
            "Epoch 176, Train Loss: 0.2536, Precision@20: 0.3661, Recall@20: 0.1970, nDCG@20: 0.4083\n",
            "Epoch 177, Train Loss: 0.2525, Precision@20: 0.3673, Recall@20: 0.1973, nDCG@20: 0.4106\n",
            "Epoch 178, Train Loss: 0.2509, Precision@20: 0.3660, Recall@20: 0.1970, nDCG@20: 0.4096\n",
            "Epoch 179, Train Loss: 0.2507, Precision@20: 0.3644, Recall@20: 0.1971, nDCG@20: 0.4072\n",
            "Epoch 180, Train Loss: 0.2505, Precision@20: 0.3624, Recall@20: 0.1963, nDCG@20: 0.4052\n",
            "Epoch 181, Train Loss: 0.2510, Precision@20: 0.3692, Recall@20: 0.1993, nDCG@20: 0.4126\n",
            "Epoch 182, Train Loss: 0.2519, Precision@20: 0.3721, Recall@20: 0.2001, nDCG@20: 0.4163\n",
            "Epoch 183, Train Loss: 0.2490, Precision@20: 0.3701, Recall@20: 0.1989, nDCG@20: 0.4144\n",
            "Epoch 184, Train Loss: 0.2455, Precision@20: 0.3664, Recall@20: 0.1972, nDCG@20: 0.4107\n",
            "Epoch 185, Train Loss: 0.2503, Precision@20: 0.3660, Recall@20: 0.1972, nDCG@20: 0.4103\n",
            "Epoch 186, Train Loss: 0.2455, Precision@20: 0.3712, Recall@20: 0.2007, nDCG@20: 0.4153\n",
            "Epoch 187, Train Loss: 0.2454, Precision@20: 0.3720, Recall@20: 0.2010, nDCG@20: 0.4161\n",
            "Epoch 188, Train Loss: 0.2440, Precision@20: 0.3709, Recall@20: 0.2002, nDCG@20: 0.4153\n",
            "Epoch 189, Train Loss: 0.2437, Precision@20: 0.3722, Recall@20: 0.2010, nDCG@20: 0.4166\n",
            "Epoch 190, Train Loss: 0.2420, Precision@20: 0.3719, Recall@20: 0.2005, nDCG@20: 0.4164\n",
            "Epoch 191, Train Loss: 0.2435, Precision@20: 0.3710, Recall@20: 0.1993, nDCG@20: 0.4150\n",
            "Epoch 192, Train Loss: 0.2424, Precision@20: 0.3693, Recall@20: 0.1982, nDCG@20: 0.4133\n",
            "Epoch 193, Train Loss: 0.2434, Precision@20: 0.3689, Recall@20: 0.1985, nDCG@20: 0.4133\n",
            "Epoch 194, Train Loss: 0.2419, Precision@20: 0.3717, Recall@20: 0.2008, nDCG@20: 0.4162\n",
            "Epoch 195, Train Loss: 0.2408, Precision@20: 0.3726, Recall@20: 0.2011, nDCG@20: 0.4173\n",
            "Epoch 196, Train Loss: 0.2441, Precision@20: 0.3752, Recall@20: 0.2034, nDCG@20: 0.4208\n",
            "Epoch 197, Train Loss: 0.2427, Precision@20: 0.3764, Recall@20: 0.2037, nDCG@20: 0.4228\n",
            "Epoch 198, Train Loss: 0.2442, Precision@20: 0.3725, Recall@20: 0.2009, nDCG@20: 0.4167\n",
            "Epoch 199, Train Loss: 0.2433, Precision@20: 0.3698, Recall@20: 0.1994, nDCG@20: 0.4150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "k=6"
      ],
      "metadata": {
        "id": "0COA_UJDhgsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "from torch_geometric.nn import SAGEConv, to_hetero\n",
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, num_layers=6, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        # Input to hidden layer\n",
        "        self.convs.append(SAGEConv((-1, -1), hidden_channels))\n",
        "\n",
        "        # Hidden to hidden layers\n",
        "        for _ in range(1, num_layers - 1):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "\n",
        "        # Hidden to output layer\n",
        "        self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "            x = self.dropout(x)  # Apply dropout after each hidden layer\n",
        "\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, data: HeteroData, hidden_channels):\n",
        "        super().__init__()\n",
        "        # Extract the number of nodes for users and movies\n",
        "        num_users = data['user'].num_nodes\n",
        "        num_movies = data['movie'].num_nodes\n",
        "        user_feat_dim = data['user'].x.size(1)\n",
        "        movie_feat_dim = data['movie'].x.size(1)\n",
        "\n",
        "        # Learnable embeddings for users and movies\n",
        "        self.user_emb = torch.nn.Embedding(num_users, hidden_channels)\n",
        "        self.movie_emb = torch.nn.Embedding(num_movies, hidden_channels)\n",
        "\n",
        "        # Project user and movie features to the hidden dimension\n",
        "        self.user_lin = torch.nn.Linear(user_feat_dim, hidden_channels)\n",
        "        self.movie_lin = torch.nn.Linear(movie_feat_dim, hidden_channels)\n",
        "\n",
        "        self.gnn = GNN(hidden_channels)\n",
        "\n",
        "        self.gnn = to_hetero(self.gnn, metadata=data.metadata())\n",
        "\n",
        "    def forward(self, data: HeteroData) -> dict:\n",
        "        # Generate initial embeddings for users and movies\n",
        "        x_dict = {\n",
        "            \"user\": self.user_lin(data[\"user\"].x) + self.user_emb.weight,  # Combine user features and embeddings\n",
        "            \"movie\": self.movie_lin(data[\"movie\"].x) + self.movie_emb.weight,  # Combine movie features and embeddings\n",
        "        }\n",
        "\n",
        "        # Apply the heterogeneous GNN\n",
        "        x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
        "        return x_dict\n",
        "\n",
        "\n",
        "def bpr_loss(user_emb, pos_movie_emb, neg_movie_emb):\n",
        "    pos_scores = (user_emb * pos_movie_emb).sum(dim=-1)\n",
        "    neg_scores = (user_emb * neg_movie_emb).sum(dim=-1)\n",
        "    loss = -torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def train_bpr(data: HeteroData, model, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    x_dict = model(data)\n",
        "    user_emb = x_dict['user']\n",
        "    movie_emb = x_dict['movie']\n",
        "\n",
        "    # Positive and negative edge sampling\n",
        "    pos_edge_index = data['user', 'rates', 'movie'].edge_index\n",
        "    neg_edge_index = torch.randint(\n",
        "        low=0, high=data['movie'].num_nodes, size=pos_edge_index.size(), device=pos_edge_index.device\n",
        "    )\n",
        "\n",
        "    # Extract embeddings for positive and negative edges\n",
        "    user_emb = user_emb[pos_edge_index[0]]\n",
        "    pos_movie_emb = movie_emb[pos_edge_index[1]]\n",
        "    neg_movie_emb = movie_emb[neg_edge_index[1]]\n",
        "\n",
        "    # Compute BPR loss\n",
        "    loss = bpr_loss(user_emb, pos_movie_emb, neg_movie_emb)\n",
        "    loss.backward()\n",
        "    #print(\"User Embedding Gradients:\", model.user_emb.weight.grad)\n",
        "    #print(\"Movie Embedding Gradients:\", model.movie_emb.weight.grad)\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(data: HeteroData, model, k=20):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_dict = model(data)\n",
        "        user_emb = x_dict['user']\n",
        "        movie_emb = x_dict['movie']\n",
        "\n",
        "        # Extract edge indices for positive edges\n",
        "        pos_edge_index = data['user', 'rates', 'movie'].edge_index\n",
        "\n",
        "        # Compute scores for all movies for each user\n",
        "        scores = torch.matmul(user_emb, movie_emb.t())  # [num_users, num_movies]\n",
        "\n",
        "        # Extract ground-truth positive edges\n",
        "        user_ids = pos_edge_index[0]  # Users in positive edges\n",
        "        movie_ids = pos_edge_index[1]  # Movies in positive edges\n",
        "\n",
        "        # Initialize metrics\n",
        "        precision = 0.0\n",
        "        recall = 0.0\n",
        "        ndcg = 0.0\n",
        "\n",
        "        for user_id in user_ids.unique():\n",
        "            user_scores = scores[user_id]  # Scores for the current user [num_movies]\n",
        "            true_movie_ids = movie_ids[user_ids == user_id]  # Ground truth movies for the user\n",
        "\n",
        "            # Skip users with no positive edges\n",
        "            if true_movie_ids.numel() == 0:\n",
        "                continue\n",
        "\n",
        "            # Get top-K movie indices\n",
        "            _, top_k_indices = torch.topk(user_scores, k)\n",
        "\n",
        "            # Compute Precision@K\n",
        "            hits = torch.isin(top_k_indices, true_movie_ids).sum().item()\n",
        "            precision += hits / k\n",
        "\n",
        "            # Compute Recall@K\n",
        "            recall += hits / true_movie_ids.numel()\n",
        "\n",
        "            # Compute nDCG@K\n",
        "            gains = torch.isin(top_k_indices, true_movie_ids).float()\n",
        "            discounts = torch.log2(torch.arange(2, k + 2).float())\n",
        "            dcg = (gains / discounts).sum()\n",
        "            idcg = (1.0 / discounts[: min(k, true_movie_ids.numel())]).sum()\n",
        "            ndcg += dcg / idcg\n",
        "\n",
        "        # Average metrics over all users\n",
        "        num_users = user_ids.unique().numel()\n",
        "        precision /= num_users\n",
        "        recall /= num_users\n",
        "        ndcg /= num_users\n",
        "\n",
        "    return precision, recall, ndcg\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "hidden_channels = 64\n",
        "model = Model(data, hidden_channels=hidden_channels)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_bpr(train_data, model, optimizer)\n",
        "    precision, recall, ndcg = evaluate(val_data, model, k=20)\n",
        "\n",
        "    print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Precision@20: {precision:.4f}, Recall@20: {recall:.4f}, nDCG@20: {ndcg:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fFjwdocgg1_",
        "outputId": "ecfb540e-42d7-420a-cc2d-9ffa5048fcd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Train Loss: 0.6924, Precision@20: 0.0211, Recall@20: 0.0058, nDCG@20: 0.0172\n",
            "Epoch 1, Train Loss: 0.6887, Precision@20: 0.0276, Recall@20: 0.0106, nDCG@20: 0.0213\n",
            "Epoch 2, Train Loss: 0.6857, Precision@20: 0.0297, Recall@20: 0.0138, nDCG@20: 0.0230\n",
            "Epoch 3, Train Loss: 0.6805, Precision@20: 0.0145, Recall@20: 0.0064, nDCG@20: 0.0121\n",
            "Epoch 4, Train Loss: 0.6742, Precision@20: 0.0169, Recall@20: 0.0074, nDCG@20: 0.0133\n",
            "Epoch 5, Train Loss: 0.6624, Precision@20: 0.0151, Recall@20: 0.0062, nDCG@20: 0.0116\n",
            "Epoch 6, Train Loss: 0.6504, Precision@20: 0.0112, Recall@20: 0.0050, nDCG@20: 0.0088\n",
            "Epoch 7, Train Loss: 0.6375, Precision@20: 0.0104, Recall@20: 0.0034, nDCG@20: 0.0082\n",
            "Epoch 8, Train Loss: 0.6121, Precision@20: 0.0094, Recall@20: 0.0031, nDCG@20: 0.0075\n",
            "Epoch 9, Train Loss: 0.6224, Precision@20: 0.0266, Recall@20: 0.0086, nDCG@20: 0.0198\n",
            "Epoch 10, Train Loss: 0.5999, Precision@20: 0.0350, Recall@20: 0.0126, nDCG@20: 0.0275\n",
            "Epoch 11, Train Loss: 0.6047, Precision@20: 0.0539, Recall@20: 0.0176, nDCG@20: 0.0427\n",
            "Epoch 12, Train Loss: 0.5923, Precision@20: 0.0617, Recall@20: 0.0200, nDCG@20: 0.0482\n",
            "Epoch 13, Train Loss: 0.5640, Precision@20: 0.0552, Recall@20: 0.0185, nDCG@20: 0.0415\n",
            "Epoch 14, Train Loss: 0.5510, Precision@20: 0.0574, Recall@20: 0.0181, nDCG@20: 0.0423\n",
            "Epoch 15, Train Loss: 0.5428, Precision@20: 0.0515, Recall@20: 0.0163, nDCG@20: 0.0368\n",
            "Epoch 16, Train Loss: 0.5380, Precision@20: 0.0350, Recall@20: 0.0109, nDCG@20: 0.0258\n",
            "Epoch 17, Train Loss: 0.5096, Precision@20: 0.0540, Recall@20: 0.0169, nDCG@20: 0.0411\n",
            "Epoch 18, Train Loss: 0.5062, Precision@20: 0.1187, Recall@20: 0.0499, nDCG@20: 0.0914\n",
            "Epoch 19, Train Loss: 0.5045, Precision@20: 0.1309, Recall@20: 0.0538, nDCG@20: 0.1124\n",
            "Epoch 20, Train Loss: 0.4925, Precision@20: 0.1628, Recall@20: 0.0680, nDCG@20: 0.1476\n",
            "Epoch 21, Train Loss: 0.4874, Precision@20: 0.1674, Recall@20: 0.0709, nDCG@20: 0.1735\n",
            "Epoch 22, Train Loss: 0.4798, Precision@20: 0.1809, Recall@20: 0.0767, nDCG@20: 0.1866\n",
            "Epoch 23, Train Loss: 0.4727, Precision@20: 0.1938, Recall@20: 0.0884, nDCG@20: 0.1958\n",
            "Epoch 24, Train Loss: 0.4644, Precision@20: 0.1858, Recall@20: 0.0869, nDCG@20: 0.1888\n",
            "Epoch 25, Train Loss: 0.4607, Precision@20: 0.1880, Recall@20: 0.0923, nDCG@20: 0.1877\n",
            "Epoch 26, Train Loss: 0.4559, Precision@20: 0.1983, Recall@20: 0.0977, nDCG@20: 0.1956\n",
            "Epoch 27, Train Loss: 0.4515, Precision@20: 0.2080, Recall@20: 0.1039, nDCG@20: 0.2042\n",
            "Epoch 28, Train Loss: 0.4476, Precision@20: 0.2080, Recall@20: 0.1039, nDCG@20: 0.2079\n",
            "Epoch 29, Train Loss: 0.4426, Precision@20: 0.2186, Recall@20: 0.1125, nDCG@20: 0.2170\n",
            "Epoch 30, Train Loss: 0.4349, Precision@20: 0.2186, Recall@20: 0.1125, nDCG@20: 0.2354\n",
            "Epoch 31, Train Loss: 0.4333, Precision@20: 0.2186, Recall@20: 0.1125, nDCG@20: 0.2306\n",
            "Epoch 32, Train Loss: 0.4289, Precision@20: 0.2186, Recall@20: 0.1125, nDCG@20: 0.2228\n",
            "Epoch 33, Train Loss: 0.4286, Precision@20: 0.2233, Recall@20: 0.1148, nDCG@20: 0.2392\n",
            "Epoch 34, Train Loss: 0.4251, Precision@20: 0.2318, Recall@20: 0.1183, nDCG@20: 0.2551\n",
            "Epoch 35, Train Loss: 0.4153, Precision@20: 0.2315, Recall@20: 0.1183, nDCG@20: 0.2580\n",
            "Epoch 36, Train Loss: 0.4197, Precision@20: 0.2264, Recall@20: 0.1145, nDCG@20: 0.2582\n",
            "Epoch 37, Train Loss: 0.4133, Precision@20: 0.2176, Recall@20: 0.1092, nDCG@20: 0.2525\n",
            "Epoch 38, Train Loss: 0.4122, Precision@20: 0.2225, Recall@20: 0.1050, nDCG@20: 0.2555\n",
            "Epoch 39, Train Loss: 0.4072, Precision@20: 0.2233, Recall@20: 0.1035, nDCG@20: 0.2556\n",
            "Epoch 40, Train Loss: 0.4076, Precision@20: 0.2280, Recall@20: 0.1060, nDCG@20: 0.2583\n",
            "Epoch 41, Train Loss: 0.4089, Precision@20: 0.2275, Recall@20: 0.1076, nDCG@20: 0.2612\n",
            "Epoch 42, Train Loss: 0.4075, Precision@20: 0.2275, Recall@20: 0.1076, nDCG@20: 0.2609\n",
            "Epoch 43, Train Loss: 0.4077, Precision@20: 0.2353, Recall@20: 0.1128, nDCG@20: 0.2671\n",
            "Epoch 44, Train Loss: 0.4047, Precision@20: 0.2411, Recall@20: 0.1197, nDCG@20: 0.2699\n",
            "Epoch 45, Train Loss: 0.4077, Precision@20: 0.2381, Recall@20: 0.1157, nDCG@20: 0.2697\n",
            "Epoch 46, Train Loss: 0.3972, Precision@20: 0.2403, Recall@20: 0.1212, nDCG@20: 0.2718\n",
            "Epoch 47, Train Loss: 0.3989, Precision@20: 0.2411, Recall@20: 0.1213, nDCG@20: 0.2722\n",
            "Epoch 48, Train Loss: 0.3987, Precision@20: 0.2417, Recall@20: 0.1235, nDCG@20: 0.2726\n",
            "Epoch 49, Train Loss: 0.3913, Precision@20: 0.2414, Recall@20: 0.1240, nDCG@20: 0.2724\n",
            "Epoch 50, Train Loss: 0.3989, Precision@20: 0.2416, Recall@20: 0.1241, nDCG@20: 0.2725\n",
            "Epoch 51, Train Loss: 0.3929, Precision@20: 0.2415, Recall@20: 0.1241, nDCG@20: 0.2721\n",
            "Epoch 52, Train Loss: 0.3949, Precision@20: 0.2403, Recall@20: 0.1236, nDCG@20: 0.2711\n",
            "Epoch 53, Train Loss: 0.3946, Precision@20: 0.2403, Recall@20: 0.1236, nDCG@20: 0.2713\n",
            "Epoch 54, Train Loss: 0.3889, Precision@20: 0.2403, Recall@20: 0.1236, nDCG@20: 0.2716\n",
            "Epoch 55, Train Loss: 0.3914, Precision@20: 0.2418, Recall@20: 0.1213, nDCG@20: 0.2722\n",
            "Epoch 56, Train Loss: 0.3912, Precision@20: 0.2433, Recall@20: 0.1217, nDCG@20: 0.2751\n",
            "Epoch 57, Train Loss: 0.3893, Precision@20: 0.2436, Recall@20: 0.1194, nDCG@20: 0.2753\n",
            "Epoch 58, Train Loss: 0.3915, Precision@20: 0.2436, Recall@20: 0.1194, nDCG@20: 0.2750\n",
            "Epoch 59, Train Loss: 0.3881, Precision@20: 0.2436, Recall@20: 0.1194, nDCG@20: 0.2750\n",
            "Epoch 60, Train Loss: 0.3903, Precision@20: 0.2436, Recall@20: 0.1194, nDCG@20: 0.2747\n",
            "Epoch 61, Train Loss: 0.3866, Precision@20: 0.2436, Recall@20: 0.1194, nDCG@20: 0.2747\n",
            "Epoch 62, Train Loss: 0.3894, Precision@20: 0.2436, Recall@20: 0.1194, nDCG@20: 0.2747\n",
            "Epoch 63, Train Loss: 0.3886, Precision@20: 0.2460, Recall@20: 0.1211, nDCG@20: 0.2761\n",
            "Epoch 64, Train Loss: 0.3860, Precision@20: 0.2462, Recall@20: 0.1212, nDCG@20: 0.2762\n",
            "Epoch 65, Train Loss: 0.3891, Precision@20: 0.2461, Recall@20: 0.1212, nDCG@20: 0.2764\n",
            "Epoch 66, Train Loss: 0.3858, Precision@20: 0.2462, Recall@20: 0.1212, nDCG@20: 0.2769\n",
            "Epoch 67, Train Loss: 0.3873, Precision@20: 0.2461, Recall@20: 0.1212, nDCG@20: 0.2771\n",
            "Epoch 68, Train Loss: 0.3852, Precision@20: 0.2461, Recall@20: 0.1212, nDCG@20: 0.2761\n",
            "Epoch 69, Train Loss: 0.3850, Precision@20: 0.2461, Recall@20: 0.1212, nDCG@20: 0.2751\n",
            "Epoch 70, Train Loss: 0.3824, Precision@20: 0.2445, Recall@20: 0.1227, nDCG@20: 0.2742\n",
            "Epoch 71, Train Loss: 0.3854, Precision@20: 0.2446, Recall@20: 0.1226, nDCG@20: 0.2741\n",
            "Epoch 72, Train Loss: 0.3866, Precision@20: 0.2439, Recall@20: 0.1232, nDCG@20: 0.2738\n",
            "Epoch 73, Train Loss: 0.3842, Precision@20: 0.2441, Recall@20: 0.1229, nDCG@20: 0.2741\n",
            "Epoch 74, Train Loss: 0.3848, Precision@20: 0.2440, Recall@20: 0.1229, nDCG@20: 0.2739\n",
            "Epoch 75, Train Loss: 0.3838, Precision@20: 0.2438, Recall@20: 0.1218, nDCG@20: 0.2734\n",
            "Epoch 76, Train Loss: 0.3849, Precision@20: 0.2457, Recall@20: 0.1205, nDCG@20: 0.2744\n",
            "Epoch 77, Train Loss: 0.3813, Precision@20: 0.2466, Recall@20: 0.1207, nDCG@20: 0.2764\n",
            "Epoch 78, Train Loss: 0.3819, Precision@20: 0.2505, Recall@20: 0.1227, nDCG@20: 0.2792\n",
            "Epoch 79, Train Loss: 0.3808, Precision@20: 0.2503, Recall@20: 0.1226, nDCG@20: 0.2794\n",
            "Epoch 80, Train Loss: 0.3814, Precision@20: 0.2492, Recall@20: 0.1219, nDCG@20: 0.2784\n",
            "Epoch 81, Train Loss: 0.3831, Precision@20: 0.2492, Recall@20: 0.1218, nDCG@20: 0.2788\n",
            "Epoch 82, Train Loss: 0.3810, Precision@20: 0.2506, Recall@20: 0.1220, nDCG@20: 0.2800\n",
            "Epoch 83, Train Loss: 0.3848, Precision@20: 0.2482, Recall@20: 0.1209, nDCG@20: 0.2784\n",
            "Epoch 84, Train Loss: 0.3825, Precision@20: 0.2475, Recall@20: 0.1206, nDCG@20: 0.2774\n",
            "Epoch 85, Train Loss: 0.3775, Precision@20: 0.2467, Recall@20: 0.1191, nDCG@20: 0.2766\n",
            "Epoch 86, Train Loss: 0.3835, Precision@20: 0.2481, Recall@20: 0.1210, nDCG@20: 0.2784\n",
            "Epoch 87, Train Loss: 0.3809, Precision@20: 0.2471, Recall@20: 0.1198, nDCG@20: 0.2777\n",
            "Epoch 88, Train Loss: 0.3856, Precision@20: 0.2468, Recall@20: 0.1196, nDCG@20: 0.2769\n",
            "Epoch 89, Train Loss: 0.3803, Precision@20: 0.2479, Recall@20: 0.1219, nDCG@20: 0.2774\n",
            "Epoch 90, Train Loss: 0.3843, Precision@20: 0.2484, Recall@20: 0.1223, nDCG@20: 0.2785\n",
            "Epoch 91, Train Loss: 0.3791, Precision@20: 0.2494, Recall@20: 0.1229, nDCG@20: 0.2794\n",
            "Epoch 92, Train Loss: 0.3850, Precision@20: 0.2484, Recall@20: 0.1222, nDCG@20: 0.2783\n",
            "Epoch 93, Train Loss: 0.3864, Precision@20: 0.2486, Recall@20: 0.1223, nDCG@20: 0.2783\n",
            "Epoch 94, Train Loss: 0.3802, Precision@20: 0.2500, Recall@20: 0.1223, nDCG@20: 0.2796\n",
            "Epoch 95, Train Loss: 0.3816, Precision@20: 0.2505, Recall@20: 0.1223, nDCG@20: 0.2803\n",
            "Epoch 96, Train Loss: 0.3837, Precision@20: 0.2498, Recall@20: 0.1220, nDCG@20: 0.2796\n",
            "Epoch 97, Train Loss: 0.3779, Precision@20: 0.2453, Recall@20: 0.1158, nDCG@20: 0.2757\n",
            "Epoch 98, Train Loss: 0.3784, Precision@20: 0.2482, Recall@20: 0.1182, nDCG@20: 0.2783\n",
            "Epoch 99, Train Loss: 0.3802, Precision@20: 0.2506, Recall@20: 0.1222, nDCG@20: 0.2809\n",
            "Epoch 100, Train Loss: 0.3783, Precision@20: 0.2496, Recall@20: 0.1207, nDCG@20: 0.2798\n",
            "Epoch 101, Train Loss: 0.3763, Precision@20: 0.2463, Recall@20: 0.1167, nDCG@20: 0.2770\n",
            "Epoch 102, Train Loss: 0.3777, Precision@20: 0.2494, Recall@20: 0.1204, nDCG@20: 0.2794\n",
            "Epoch 103, Train Loss: 0.3824, Precision@20: 0.2508, Recall@20: 0.1224, nDCG@20: 0.2816\n",
            "Epoch 104, Train Loss: 0.3771, Precision@20: 0.2507, Recall@20: 0.1224, nDCG@20: 0.2810\n",
            "Epoch 105, Train Loss: 0.3819, Precision@20: 0.2498, Recall@20: 0.1215, nDCG@20: 0.2801\n",
            "Epoch 106, Train Loss: 0.3769, Precision@20: 0.2497, Recall@20: 0.1209, nDCG@20: 0.2799\n",
            "Epoch 107, Train Loss: 0.3770, Precision@20: 0.2502, Recall@20: 0.1219, nDCG@20: 0.2806\n",
            "Epoch 108, Train Loss: 0.3784, Precision@20: 0.2503, Recall@20: 0.1221, nDCG@20: 0.2810\n",
            "Epoch 109, Train Loss: 0.3827, Precision@20: 0.2495, Recall@20: 0.1203, nDCG@20: 0.2796\n",
            "Epoch 110, Train Loss: 0.3746, Precision@20: 0.2476, Recall@20: 0.1176, nDCG@20: 0.2778\n",
            "Epoch 111, Train Loss: 0.3801, Precision@20: 0.2498, Recall@20: 0.1219, nDCG@20: 0.2799\n",
            "Epoch 112, Train Loss: 0.3758, Precision@20: 0.2506, Recall@20: 0.1223, nDCG@20: 0.2807\n",
            "Epoch 113, Train Loss: 0.3774, Precision@20: 0.2511, Recall@20: 0.1226, nDCG@20: 0.2811\n",
            "Epoch 114, Train Loss: 0.3730, Precision@20: 0.2488, Recall@20: 0.1206, nDCG@20: 0.2793\n",
            "Epoch 115, Train Loss: 0.3783, Precision@20: 0.2457, Recall@20: 0.1163, nDCG@20: 0.2766\n",
            "Epoch 116, Train Loss: 0.3787, Precision@20: 0.2462, Recall@20: 0.1168, nDCG@20: 0.2774\n",
            "Epoch 117, Train Loss: 0.3765, Precision@20: 0.2461, Recall@20: 0.1166, nDCG@20: 0.2772\n",
            "Epoch 118, Train Loss: 0.3748, Precision@20: 0.2458, Recall@20: 0.1162, nDCG@20: 0.2769\n",
            "Epoch 119, Train Loss: 0.3746, Precision@20: 0.2448, Recall@20: 0.1152, nDCG@20: 0.2759\n",
            "Epoch 120, Train Loss: 0.3766, Precision@20: 0.2444, Recall@20: 0.1151, nDCG@20: 0.2756\n",
            "Epoch 121, Train Loss: 0.3752, Precision@20: 0.2466, Recall@20: 0.1175, nDCG@20: 0.2777\n",
            "Epoch 122, Train Loss: 0.3751, Precision@20: 0.2513, Recall@20: 0.1210, nDCG@20: 0.2805\n",
            "Epoch 123, Train Loss: 0.3762, Precision@20: 0.2517, Recall@20: 0.1236, nDCG@20: 0.2813\n",
            "Epoch 124, Train Loss: 0.3729, Precision@20: 0.2480, Recall@20: 0.1208, nDCG@20: 0.2785\n",
            "Epoch 125, Train Loss: 0.3734, Precision@20: 0.2483, Recall@20: 0.1222, nDCG@20: 0.2790\n",
            "Epoch 126, Train Loss: 0.3798, Precision@20: 0.2501, Recall@20: 0.1241, nDCG@20: 0.2809\n",
            "Epoch 127, Train Loss: 0.3742, Precision@20: 0.2489, Recall@20: 0.1241, nDCG@20: 0.2804\n",
            "Epoch 128, Train Loss: 0.3755, Precision@20: 0.2477, Recall@20: 0.1227, nDCG@20: 0.2792\n",
            "Epoch 129, Train Loss: 0.3797, Precision@20: 0.2475, Recall@20: 0.1228, nDCG@20: 0.2781\n",
            "Epoch 130, Train Loss: 0.3814, Precision@20: 0.2470, Recall@20: 0.1224, nDCG@20: 0.2778\n",
            "Epoch 131, Train Loss: 0.3747, Precision@20: 0.2488, Recall@20: 0.1232, nDCG@20: 0.2802\n",
            "Epoch 132, Train Loss: 0.3757, Precision@20: 0.2496, Recall@20: 0.1248, nDCG@20: 0.2808\n",
            "Epoch 133, Train Loss: 0.3786, Precision@20: 0.2490, Recall@20: 0.1234, nDCG@20: 0.2803\n",
            "Epoch 134, Train Loss: 0.3759, Precision@20: 0.2474, Recall@20: 0.1227, nDCG@20: 0.2780\n",
            "Epoch 135, Train Loss: 0.3778, Precision@20: 0.2477, Recall@20: 0.1225, nDCG@20: 0.2786\n",
            "Epoch 136, Train Loss: 0.3811, Precision@20: 0.2516, Recall@20: 0.1239, nDCG@20: 0.2824\n",
            "Epoch 137, Train Loss: 0.3739, Precision@20: 0.2514, Recall@20: 0.1246, nDCG@20: 0.2820\n",
            "Epoch 138, Train Loss: 0.3806, Precision@20: 0.2512, Recall@20: 0.1240, nDCG@20: 0.2817\n",
            "Epoch 139, Train Loss: 0.3753, Precision@20: 0.2475, Recall@20: 0.1191, nDCG@20: 0.2785\n",
            "Epoch 140, Train Loss: 0.3754, Precision@20: 0.2442, Recall@20: 0.1156, nDCG@20: 0.2756\n",
            "Epoch 141, Train Loss: 0.3804, Precision@20: 0.2462, Recall@20: 0.1170, nDCG@20: 0.2778\n",
            "Epoch 142, Train Loss: 0.3738, Precision@20: 0.2472, Recall@20: 0.1182, nDCG@20: 0.2778\n",
            "Epoch 143, Train Loss: 0.3749, Precision@20: 0.2479, Recall@20: 0.1197, nDCG@20: 0.2787\n",
            "Epoch 144, Train Loss: 0.3787, Precision@20: 0.2471, Recall@20: 0.1178, nDCG@20: 0.2782\n",
            "Epoch 145, Train Loss: 0.3737, Precision@20: 0.2476, Recall@20: 0.1159, nDCG@20: 0.2782\n",
            "Epoch 146, Train Loss: 0.3712, Precision@20: 0.2480, Recall@20: 0.1159, nDCG@20: 0.2782\n",
            "Epoch 147, Train Loss: 0.3797, Precision@20: 0.2483, Recall@20: 0.1171, nDCG@20: 0.2795\n",
            "Epoch 148, Train Loss: 0.3724, Precision@20: 0.2499, Recall@20: 0.1204, nDCG@20: 0.2806\n",
            "Epoch 149, Train Loss: 0.3788, Precision@20: 0.2503, Recall@20: 0.1206, nDCG@20: 0.2808\n",
            "Epoch 150, Train Loss: 0.3756, Precision@20: 0.2479, Recall@20: 0.1178, nDCG@20: 0.2793\n",
            "Epoch 151, Train Loss: 0.3721, Precision@20: 0.2466, Recall@20: 0.1154, nDCG@20: 0.2765\n",
            "Epoch 152, Train Loss: 0.3792, Precision@20: 0.2461, Recall@20: 0.1159, nDCG@20: 0.2764\n",
            "Epoch 153, Train Loss: 0.3759, Precision@20: 0.2466, Recall@20: 0.1177, nDCG@20: 0.2782\n",
            "Epoch 154, Train Loss: 0.3777, Precision@20: 0.2493, Recall@20: 0.1205, nDCG@20: 0.2804\n",
            "Epoch 155, Train Loss: 0.3777, Precision@20: 0.2501, Recall@20: 0.1207, nDCG@20: 0.2809\n",
            "Epoch 156, Train Loss: 0.3718, Precision@20: 0.2481, Recall@20: 0.1189, nDCG@20: 0.2793\n",
            "Epoch 157, Train Loss: 0.3721, Precision@20: 0.2463, Recall@20: 0.1178, nDCG@20: 0.2777\n",
            "Epoch 158, Train Loss: 0.3704, Precision@20: 0.2463, Recall@20: 0.1170, nDCG@20: 0.2780\n",
            "Epoch 159, Train Loss: 0.3726, Precision@20: 0.2472, Recall@20: 0.1181, nDCG@20: 0.2785\n",
            "Epoch 160, Train Loss: 0.3735, Precision@20: 0.2466, Recall@20: 0.1180, nDCG@20: 0.2781\n",
            "Epoch 161, Train Loss: 0.3694, Precision@20: 0.2471, Recall@20: 0.1176, nDCG@20: 0.2785\n",
            "Epoch 162, Train Loss: 0.3729, Precision@20: 0.2459, Recall@20: 0.1166, nDCG@20: 0.2775\n",
            "Epoch 163, Train Loss: 0.3745, Precision@20: 0.2450, Recall@20: 0.1146, nDCG@20: 0.2766\n",
            "Epoch 164, Train Loss: 0.3721, Precision@20: 0.2448, Recall@20: 0.1146, nDCG@20: 0.2765\n",
            "Epoch 165, Train Loss: 0.3736, Precision@20: 0.2457, Recall@20: 0.1155, nDCG@20: 0.2769\n",
            "Epoch 166, Train Loss: 0.3732, Precision@20: 0.2459, Recall@20: 0.1164, nDCG@20: 0.2772\n",
            "Epoch 167, Train Loss: 0.3735, Precision@20: 0.2469, Recall@20: 0.1171, nDCG@20: 0.2780\n",
            "Epoch 168, Train Loss: 0.3707, Precision@20: 0.2476, Recall@20: 0.1177, nDCG@20: 0.2789\n",
            "Epoch 169, Train Loss: 0.3729, Precision@20: 0.2473, Recall@20: 0.1180, nDCG@20: 0.2793\n",
            "Epoch 170, Train Loss: 0.3730, Precision@20: 0.2475, Recall@20: 0.1178, nDCG@20: 0.2794\n",
            "Epoch 171, Train Loss: 0.3739, Precision@20: 0.2478, Recall@20: 0.1182, nDCG@20: 0.2795\n",
            "Epoch 172, Train Loss: 0.3715, Precision@20: 0.2479, Recall@20: 0.1179, nDCG@20: 0.2794\n",
            "Epoch 173, Train Loss: 0.3715, Precision@20: 0.2482, Recall@20: 0.1177, nDCG@20: 0.2795\n",
            "Epoch 174, Train Loss: 0.3684, Precision@20: 0.2483, Recall@20: 0.1173, nDCG@20: 0.2792\n",
            "Epoch 175, Train Loss: 0.3701, Precision@20: 0.2475, Recall@20: 0.1166, nDCG@20: 0.2781\n",
            "Epoch 176, Train Loss: 0.3730, Precision@20: 0.2462, Recall@20: 0.1160, nDCG@20: 0.2773\n",
            "Epoch 177, Train Loss: 0.3682, Precision@20: 0.2465, Recall@20: 0.1160, nDCG@20: 0.2779\n",
            "Epoch 178, Train Loss: 0.3720, Precision@20: 0.2468, Recall@20: 0.1162, nDCG@20: 0.2782\n",
            "Epoch 179, Train Loss: 0.3711, Precision@20: 0.2472, Recall@20: 0.1168, nDCG@20: 0.2788\n",
            "Epoch 180, Train Loss: 0.3705, Precision@20: 0.2477, Recall@20: 0.1170, nDCG@20: 0.2791\n",
            "Epoch 181, Train Loss: 0.3689, Precision@20: 0.2481, Recall@20: 0.1171, nDCG@20: 0.2794\n",
            "Epoch 182, Train Loss: 0.3705, Precision@20: 0.2475, Recall@20: 0.1169, nDCG@20: 0.2788\n",
            "Epoch 183, Train Loss: 0.3694, Precision@20: 0.2489, Recall@20: 0.1181, nDCG@20: 0.2799\n",
            "Epoch 184, Train Loss: 0.3735, Precision@20: 0.2480, Recall@20: 0.1170, nDCG@20: 0.2791\n",
            "Epoch 185, Train Loss: 0.3687, Precision@20: 0.2469, Recall@20: 0.1155, nDCG@20: 0.2770\n",
            "Epoch 186, Train Loss: 0.3681, Precision@20: 0.2458, Recall@20: 0.1140, nDCG@20: 0.2756\n",
            "Epoch 187, Train Loss: 0.3681, Precision@20: 0.2476, Recall@20: 0.1162, nDCG@20: 0.2781\n",
            "Epoch 188, Train Loss: 0.3715, Precision@20: 0.2493, Recall@20: 0.1177, nDCG@20: 0.2800\n",
            "Epoch 189, Train Loss: 0.3694, Precision@20: 0.2511, Recall@20: 0.1194, nDCG@20: 0.2815\n",
            "Epoch 190, Train Loss: 0.3704, Precision@20: 0.2499, Recall@20: 0.1182, nDCG@20: 0.2809\n",
            "Epoch 191, Train Loss: 0.3726, Precision@20: 0.2498, Recall@20: 0.1181, nDCG@20: 0.2804\n",
            "Epoch 192, Train Loss: 0.3692, Precision@20: 0.2501, Recall@20: 0.1187, nDCG@20: 0.2814\n",
            "Epoch 193, Train Loss: 0.3690, Precision@20: 0.2517, Recall@20: 0.1204, nDCG@20: 0.2825\n",
            "Epoch 194, Train Loss: 0.3683, Precision@20: 0.2513, Recall@20: 0.1194, nDCG@20: 0.2824\n",
            "Epoch 195, Train Loss: 0.3664, Precision@20: 0.2508, Recall@20: 0.1189, nDCG@20: 0.2819\n",
            "Epoch 196, Train Loss: 0.3712, Precision@20: 0.2534, Recall@20: 0.1215, nDCG@20: 0.2843\n",
            "Epoch 197, Train Loss: 0.3675, Precision@20: 0.2546, Recall@20: 0.1228, nDCG@20: 0.2856\n",
            "Epoch 198, Train Loss: 0.3680, Precision@20: 0.2514, Recall@20: 0.1197, nDCG@20: 0.2826\n",
            "Epoch 199, Train Loss: 0.3692, Precision@20: 0.2512, Recall@20: 0.1192, nDCG@20: 0.2820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rFaAxU7-hqV7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}